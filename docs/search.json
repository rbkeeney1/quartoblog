[
  {
    "objectID": "helpful_links.html",
    "href": "helpful_links.html",
    "title": "Helpful Resources and Links",
    "section": "",
    "text": "LinkedIn\n  \n  \n     Twitter\n  \n  \n     GitHub\n  \n  \n     Email\n  \n\n      \nThese are Ryan‚Äôs favorite bookmarks. They cover everything from programming 101 to deep learning. If you have have a suggestion, please contact Ryan."
  },
  {
    "objectID": "helpful_links.html#data-science-resources",
    "href": "helpful_links.html#data-science-resources",
    "title": "Helpful Resources and Links",
    "section": "Data Science Resources",
    "text": "Data Science Resources\nAn Introduction to Statistical Learning\nThe Elements of Statistical Learning\nFoundations of Machine Learning"
  },
  {
    "objectID": "helpful_links.html#r-resources",
    "href": "helpful_links.html#r-resources",
    "title": "Helpful Resources and Links",
    "section": "R Resources",
    "text": "R Resources\nR cookbook\nR for Data Science\nMastering Shiny\nTidy Modeling with R. My absolute favorite way to develop models."
  },
  {
    "objectID": "helpful_links.html#python-resources",
    "href": "helpful_links.html#python-resources",
    "title": "Helpful Resources and Links",
    "section": "Python Resources",
    "text": "Python Resources\nA Whirlwind Tour of Python, by Jake VanderPlas A solid introduction to Python, assuming you are already familiar with another programming language.\nPython Data Science Handbook\nPythex | In-browser regex tester"
  },
  {
    "objectID": "helpful_links.html#data-visualization-resources",
    "href": "helpful_links.html#data-visualization-resources",
    "title": "Helpful Resources and Links",
    "section": "Data Visualization Resources",
    "text": "Data Visualization Resources\nFrom Data to Viz. A fantastic guide to all types of plots by Yan Holtz and Conor Healy. Also check out their Dataviz Inspiration page.\nBBC Visual and Data Journalism cookbook for R graphics\nCedric Scherer‚Äôs Data Visualization & Information Design"
  },
  {
    "objectID": "helpful_links.html#data-engineering-resources",
    "href": "helpful_links.html#data-engineering-resources",
    "title": "Helpful Resources and Links",
    "section": "Data Engineering Resources",
    "text": "Data Engineering Resources\nThe Ultimate Guide to Deploying a Shiny App on AWS, by Charles Bordet. I personally used the guide to launch my own EC2 servers.\nOh Shit, Git!?! You‚Äôve messed up. Bad. This resource might just save you.\nIntro to SQL: Querying and managing data"
  },
  {
    "objectID": "helpful_links.html#math-resources",
    "href": "helpful_links.html#math-resources",
    "title": "Helpful Resources and Links",
    "section": "Math Resources",
    "text": "Math Resources\nEngineering Biostatistics: An Introduction Using MATLAB and WinBUGS by Brani Vidakovic. Excellent resources for Bayesian statistics, MATLAB and WinBugs.\nWolframe|Alpha. Powerful in-browser computational aid.\nSymbolab. Another powerful in-browser computational aid. Usually provides steps to solve problems at no extra cost.\nOctave. Open source software to run those pesky .m scripts (Free alternative to MatLab)\nOpenBUGS. Free Bayesian software. I prefer to use R or Python.\nCalculus Made Easy by S. P. Thompson\nMathematics for Machine Learning\nThe Matrix Cookbook\nIntroduction to Applied Linear Algebra"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ryan Keeney",
    "section": "",
    "text": "LinkedIn\n  \n  \n     Twitter\n  \n  \n     GitHub\n  \n  \n     Email\n  \n\n      \nRyan Keeney (he/√©l) is a data scientist who enjoys helping others investigate, analyze, and build solutions for critical business decisions. He holds a graduate degree in data analytics Georgia Tech and a bachelor‚Äôs degree in mechanical engineering from the University of Michigan, Ann Arbor. When he‚Äôs not pursuing a data science project, Ryan enjoys teaching, volunteering, and traveling with his family. He is an proud ally and has been involved in projects furthering equality and access to education, healthcare, and human rights in the United States and internationally since 2013."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Ryan Keeney",
    "section": "Education",
    "text": "Education\nGeorgia Institute of Technology | Atlanta, GA\nMaster of Science in Analytics | January 2021 - December 2022 | Reference Projects\nUniversity of Michigan | Ann Arbor, MI\nBSE in Mechanical Engineering | Sept 2009 - April 2014"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Ryan Keeney",
    "section": "Experience",
    "text": "Experience\nFF Astronauts | Director of Data Analytics | June 2020 - Present\nAmerican School of Asuncion | Math and Science Teacher | July 2021 - June 2022\nJohn Deere | Design Engineer, Product Marketing Manager | May 2014 - June 2020\nAdditional Experience | Chrysler FIAT, Toyota, Commnet, Gates Corporation"
  },
  {
    "objectID": "index.html#skills-and-tools",
    "href": "index.html#skills-and-tools",
    "title": "Ryan Keeney",
    "section": "Skills and Tools",
    "text": "Skills and Tools\nInterested in working with Ryan? Connect with Ryan through email or LinkedIn.\nRyan has experience with various supervised, unsupervised, and reinforcement learning techniques and as applied various statistical and machine learning techniques to supervised, unsupervised, and reinforcement learning tasks. He also has experience in simulation, Bayesian statistics, and presenting to C-level stakeholders.\n\n\n\n\n\n\n\nSupervised Learning\nUnsupervised Learning\n\n\n\n\nLinear regression, multiple-linear regression, logistic regression, LASSO, ridge regression, elastic net, decision trees, random forest, SVM, KNN, neural nets, boosted gradient trees, kernel-based methods, time-series analysis, anomaly detection, forecasting, deep learning for image and NLP applications.\nK-Means, hierarchical clustering, spectral clustering, network graph analysis, ISOMAP, PCA, density estimation.\n\n\n\nTools: Advanced in R (Tidyverse, Tidymodels, Shiny, ggplot), Python (NumPy, Pandas, SciPy, SciKit-learn, PySpark) | Proficient in multiple SQL languages, AWS EC2 Computing, BUGS, ARENA | Experience in HTML/CSS, MATLAB, Spark, Tableau, Azure, Databricks, D3"
  },
  {
    "objectID": "index.html#languages",
    "href": "index.html#languages",
    "title": "Ryan Keeney",
    "section": "Languages",
    "text": "Languages\nFluent in English, Conversational Proficiency in Spanish"
  },
  {
    "objectID": "index.html#awards",
    "href": "index.html#awards",
    "title": "Ryan Keeney",
    "section": "Awards",
    "text": "Awards\nJohn Deere Innovation Award (2019), Serving our Communities (2016), Mike Schmidt Memorial Baja SAE (2013)"
  },
  {
    "objectID": "posts/anova_introduction.html",
    "href": "posts/anova_introduction.html",
    "title": "ANOVA | Introduction",
    "section": "",
    "text": "The goal of ANOVA is to compare the ‚Äúwithin‚Äù group variability to the ‚Äúbetween‚Äù variability of multiple samples of data for a response variable, with different categories.\nImagine the data represented as set of box plots:\n\nWithin variability: Within each region (think: individual box plot)\nBetween variability: Differences between box plots\n\nMore directly, ANOVA can be thought of comparing the means of multiple samples."
  },
  {
    "objectID": "posts/anova_introduction.html#anova-vs.-other-methods",
    "href": "posts/anova_introduction.html#anova-vs.-other-methods",
    "title": "ANOVA | Introduction",
    "section": "ANOVA vs.¬†other methods",
    "text": "ANOVA vs.¬†other methods\nANOVA is more flexible that the Student‚Äôs t test because it can used to compare the means among three or more groups.\nIt is a parametric test, and can be sensitive to P-hacking if care is not taken during experiment design or sampling.\nUnlike a TukeyHSD tests, a significant P value of the ANOVA test indicates for at least one pair, between which the mean difference was statistically significant. *However, ANOVA will not specify which group, nor how many groups, presented a statistically significant difference.\nLike simple linear regression, ANOVA uses a quantitative response variable. However, ANOVA uses a categorical predicting variable while SLR‚Äôs is quantitative. Essentially, ANOVA is a linear regression model where the predicting factor is a categorical variable. ANOVA uses dummy variables with categorial predictors. With multiple predictors, it‚Äôs a multiple linear regression.\nANOVA Assumptions:\n\nConstant variance: The variance of the error terms is constant\nIndependence: The error terms are independent random variables\nNormality: The error terms are normally distributed\n\nAdditional Material:\n\nANOVA tutorial\nANOVA Math\nUnderstanding ANOVA in R"
  },
  {
    "objectID": "posts/anova_introduction.html#example-anova-output-in-r",
    "href": "posts/anova_introduction.html#example-anova-output-in-r",
    "title": "ANOVA | Introduction",
    "section": "Example ANOVA output in R",
    "text": "Example ANOVA output in R\nJet lag is a common problem for people traveling across multiple time zones, but people can gradually adjust to the new time zone since the exposure of the shifted light schedule to their eyes can resets the internal circadian rhythm in a process called ‚Äúphase shift‚Äù. Campbell and Murphy (1998) in a highly controversial study reported that the human circadian clock can also be reset by only exposing the back of the knee to light, with some hailing this as a major discovery and others challenging aspects of the experimental design. The table below is taken from a later experiment by Wright and Czeisler (2002) that re-examined the phenomenon. The new experiment measured circadian rhythm through the daily cycle of melatonin production in 22 subjects randomly assigned to one of three light treatments. Subjects were woken from sleep and for three hours were exposed to bright lights applied to the eyes only, to the knees only or to neither (control group). The effects of treatment to the circadian rhythm were measured two days later by the magnitude of phase shift (measured in hours) in each subject‚Äôs daily cycle of melatonin production. A negative measurement indicates a delay in melatonin production, a predicted effect of light treatment, while a positive number indicates an advance.\nRaw data of phase shift, in hours, for the circadian rhythm experiment.\n\n\n\nTreatment\nPhase Shift (hr)\n\n\n\n\nControl\n0.53, 0.36, 0.20, -0.37, -0.60, -0.64, -0.68, -1.27\n\n\nKnees\n0.73, 0.31, 0.03, -0.29, -0.56, -0.96, -1.61\n\n\nEyes\n-0.78, -0.86, -1.35, -1.48, -1.52, -2.04, -2.83\n\n\n\nRunning an ANOVA data in R results in the following output. You can read the ‚Äútreatments‚Äù as the ‚Äúbetween‚Äù variability and the ‚Äúerror‚Äù as the ‚Äúwithin‚Äù variability - the ratio between the two are then compared in the F-test.\n\n\n\n\n\n\n\n\n\n\n\nSource\nDf\nSum of Squares\nMean Squares\nF-statistics\np-value\n\n\n\n\nTreatments\n2\n7.2244\n3.6122\n7.29\n0.004\n\n\nError\n19\n9.415\n0.4955\n\n\n\n\nTOTAL\n21\n16.6394\n\n\n\n\n\n\nFor reference, here is the same table with the parameters named.\n\n\n\n\n\n\n\n\n\n\n\nSource\nDf\nSum of Squares\nMean Squares\nF-statistics\np-value\n\n\n\n\nTreatments\n(between)\n\\(k-1\\)\n\\(SSTR\\)\n\\(MSTR=\\)\n\\(STR/ (k-1)\\)\n\\(F= MSTR / MSE\\)\n*\n\n\nError\n(within)\n\\(N-k\\)\n\\(SSE\\)\n\\(MSE = SSE / (N-k)\\)\n\n\n\n\nTOTAL\n\\(N-1\\)\n\\(SST\\)\n\n\n\n\n\n\nWhere:\n\nk = 3 (number of groups)\nN = 22 (number o data points)\nTreatment Df = K-1 = 3-1 = 2\nError Df = N-k = 22 - 3 = 19\nTOTAL Df = 2 and 19\nSSTR = MSTR * Df = 3.6122 * 2 = 7.2244\nMSE = SSE/Df = 9.415 / 19 = 0.4955\nSST = SSE + SSTR = 7.2244 + 9.415 = 16.6394\nF-statistic = MSTR/MSE = 3.6122 / 0.4955 = 7.29\n\n\nGroup Means\n\n\\(\\mu_1\\): estimated mean of the treatment group ‚Äúcontrol‚Äù = -0.31\n\\(\\mu_2\\): estimated mean of the treatment group ‚Äúknees‚Äù = -0.36\n\\(\\mu_3\\): estimated mean of the treatment group ‚Äúeyes‚Äù = -1.55\n\n\n\nNull hypothesis of the ANOVA \\(F\\)-test, \\(H_0\\)\n\\(H_0\\): All the means of treatment categories are the same.\n\n\nAlternative hypothesis of the ANOVA \\(F\\)-test, \\(H_A\\)\n\\(H_A\\): At least one pair of treatment categories have different means.\n\n\nF-test, p-value, and results\nThe resulting p-value from the F-test \\(F\\)(k-1, N-k) or \\(F_{(2,19)}\\) is 0.004. At the level of \\(\\alpha\\)-level of 0.05, we reject the null hypothesis, so at least 1 pair of the treatment groups‚Äô means are likely different (statistically significant). However, without pairwise comparison, we do not know which pair(s) are different. Additional analysis is needed to evaluate the control group vs the knee and eye treatment groups.\n\n\nR Code\nSince the samples are different length, I use a quick trick of combining them into a single tibble with a ‚Äúname‚Äù and ‚Äúvalue‚Äù column.\n\nlibrary(dplyr)\nlibrary(ggplot2)\n\nControl = tibble(\n  name = \"Control\",\n  value = c(0.53, 0.36, 0.20, -0.37, -0.60, -0.64, -0.68, -1.27)\n)\n\nKnees = tibble(\n  name = \"Knees\",\n  value = c(0.73, 0.31, 0.03, -0.29, -0.56, -0.96, -1.61)\n)\n\n\nEyes = tibble(\n  name = \"Eyes\",\n  value = c(-0.78, -0.86, -1.35, -1.48, -1.52, -2.04, -2.83)\n)\n\n\n# bind into tibble  \ndata = rbind(Control,Knees) |> rbind(Eyes)\n\ndata |> sample_n(5) # randomly show 5 rows\n\n# A tibble: 5 x 2\n  name    value\n  <chr>   <dbl>\n1 Control  0.36\n2 Control -0.68\n3 Control  0.53\n4 Knees   -0.56\n5 Eyes    -1.48\n\n\nWith this done, we can quickly check the means.\n\ndata |>  group_by(name) |> dplyr::summarise(n = n(), mean=mean(value), var = var(value))\n\n# A tibble: 3 x 4\n  name        n   mean   var\n  <chr>   <int>  <dbl> <dbl>\n1 Control     8 -0.309 0.381\n2 Eyes        7 -1.55  0.499\n3 Knees       7 -0.336 0.625\n\n\nMake a quick boxplot using ggplot.\n\ndata |>\n  ggplot(aes(x=name, y=value, fill = name))+\n  geom_boxplot()+\n  geom_jitter(size=2, alpha = 0.5,width = 0.2)+ \n  theme_bw() +\n  theme(\n      legend.position=\"none\",\n      ) +\n  labs(\n    title = \"Boxplot\",\n    x = \"\"\n  )\n\n\n\n\nAnd finally run ANOVA.\n\naov_model <- aov(value~name, data = data)\naov_model\n\nCall:\n   aov(formula = value ~ name, data = data)\n\nTerms:\n                    name Residuals\nSum of Squares  7.224492  9.415345\nDeg. of Freedom        2        19\n\nResidual standard error: 0.7039492\nEstimated effects may be unbalanced\n\n\n\naov_model |> summary()\n\n            Df Sum Sq Mean Sq F value  Pr(>F)   \nname         2  7.224   3.612   7.289 0.00447 **\nResiduals   19  9.415   0.496                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nChecking the fit of the model, we see that all three assumptions hold, but note there are a few points identified as possible outliers.\n\nplot(aov_model)\n\n\n\n\n\n\n\n\n\n\n\n\n\nRemember, ANOVA doesn‚Äôt tell us which pair is different, only if a pair exists with statistically different means.\nWe can use TukeyHSD to identify which groups are different to the others through pair-wise comparison. Eyes is determined to have a mean that is statistically different than the Control and Knees groups while the Control and Knees groups do not statistically different means.\n\nTukeyHSD(aov_model)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = value ~ name, data = data)\n\n$name\n                     diff        lwr        upr     p adj\nEyes-Control  -1.24267857 -2.1682364 -0.3171207 0.0078656\nKnees-Control -0.02696429 -0.9525222  0.8985936 0.9969851\nKnees-Eyes     1.21571429  0.2598022  2.1716263 0.0116776"
  },
  {
    "objectID": "posts/avatar_vader.html",
    "href": "posts/avatar_vader.html",
    "title": "Avatar VADER Sentiment Analysis",
    "section": "",
    "text": "The August 12, 2020 TidyTuesday dataset presents a perfect opportunity to try out the {vader} package which can be used for native language processing.\nMy goal was to analyze character sentiment by breaking down each character‚Äôs lines and running them through vader. Here are a few questions that I hoped to answer along the way:\n\nWho are the most positive characters?\nWho are the most negative characters?\nDoes character sentiment change over the duration of the series?\n\nI also wanted to explore a few new concepts such as the unique piping operators in {magrittr}, {tvthemes}, fonts, customizing themes, and improving my plot annotations. And while I wanted to make a word cloud in the shape of Appa - that‚Äôs going to have to wait until next time."
  },
  {
    "objectID": "posts/avatar_vader.html#initial-libraries-and-load-data",
    "href": "posts/avatar_vader.html#initial-libraries-and-load-data",
    "title": "Avatar VADER Sentiment Analysis",
    "section": "Initial libraries and load data",
    "text": "Initial libraries and load data\nThe first set of libraries to load are as follows. I do load a few font and theme libraries later. But, if you want to use vader this will get you started.\n\nlibrary(tidytuesdayR) # to get data quickly\nlibrary(tidyverse) # no matter how dark, there is always hope\nlibrary(magrittr) # for some more unique piping options\nlibrary(sentimentr) # for getting sentences\nlibrary(vader) # for vader sentiment scores\n\nAs always, TidyTuesday makes it easy to get the data.\n\ntuesdata <- tidytuesdayR::tt_load('2020-08-11')\n\n--- Compiling #TidyTuesday Information for 2020-08-11 ----\n\n\n--- There are 2 files available ---\n\n\n--- Starting Download ---\n\n\n\n    Downloading file 1 of 2: `avatar.csv`\n    Downloading file 2 of 2: `scene_description.csv`\n\n\n--- Download complete ---\n\navatar <- tuesdata$avatar\n\nLet‚Äôs take a quick look at what is included in this dataset. It‚Äôs the script to each episode, including the writer, director, and IMDB episode - however, we‚Äôre going to focus on what the characters are speaking.\n\ntibble::tribble(\n             ~variable,      ~class,                                    ~description,\n                  \"id\",   \"integer\",                         \"Unique Row identifier\",\n                \"book\", \"character\",                                     \"Book name\",\n            \"book_num\",   \"integer\",                                   \"Book number\",\n             \"chapter\", \"character\",                                  \"Chapter name\",\n         \"chapter_num\",   \"integer\",                                  \"Chapter Name\",\n           \"character\", \"character\",                            \"Character speaking\",\n           \"full_text\", \"character\", \"Full text (scene description, character text)\",\n     \"character_words\", \"character\",                   \"Text coming from characters\",\n              \"writer\", \"character\",                                \"Writer of book\",\n            \"director\", \"character\",                           \"Director of episode\",\n         \"imdb_rating\",    \"double\",                       \"IMDB rating for episode\"\n     ) %>% knitr::kable()\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nid\ninteger\nUnique Row identifier\n\n\nbook\ncharacter\nBook name\n\n\nbook_num\ninteger\nBook number\n\n\nchapter\ncharacter\nChapter name\n\n\nchapter_num\ninteger\nChapter Name\n\n\ncharacter\ncharacter\nCharacter speaking\n\n\nfull_text\ncharacter\nFull text (scene description, character text)\n\n\ncharacter_words\ncharacter\nText coming from characters\n\n\nwriter\ncharacter\nWriter of book\n\n\ndirector\ncharacter\nDirector of episode\n\n\nimdb_rating\ndouble\nIMDB rating for episode"
  },
  {
    "objectID": "posts/avatar_vader.html#who-speaks-the-most-often",
    "href": "posts/avatar_vader.html#who-speaks-the-most-often",
    "title": "Avatar VADER Sentiment Analysis",
    "section": "Who speaks the most often?",
    "text": "Who speaks the most often?\nFirst, I ran a quick line of code to see who talked the most often and if I need to clean it up a bit. Looks like I'm going to have to remove the \"Scene Description\". Additionally, I plan on picking a subset of the characters to plot later and I want to see who speaks enough to be included.\n\navatar %>% group_by(character) %>% summarize(n=n())%>% arrange(-n) %>% head(20) %>% knitr::kable()\n\n\n\n\ncharacter\nn\n\n\n\n\nScene Description\n3393\n\n\nAang\n1796\n\n\nSokka\n1639\n\n\nKatara\n1437\n\n\nZuko\n776\n\n\nToph\n507\n\n\nIroh\n337\n\n\nAzula\n211\n\n\nJet\n134\n\n\nSuki\n114\n\n\nZhao\n107\n\n\nMai\n82\n\n\nHakoda\n77\n\n\nRoku\n67\n\n\nTy Lee\n64\n\n\nOzai\n59\n\n\nBumi\n55\n\n\nYue\n53\n\n\nHama\n49\n\n\nWarden\n49"
  },
  {
    "objectID": "posts/avatar_vader.html#data-prep",
    "href": "posts/avatar_vader.html#data-prep",
    "title": "Avatar VADER Sentiment Analysis",
    "section": "Data Prep",
    "text": "Data Prep\nThanks to Avery Robbins for dataset {appa} there isn't much cleaning do be done, I only removed the \"Scene Description\". Since I'm also trying to get used to using the magrittr %<>% assignment-piping operator, this was a is was a good opportunity to start.\n\navatar_clean <- avatar \navatar_clean %<>% filter(character != \"Scene Description\") %>% select(id:character,character_words)"
  },
  {
    "objectID": "posts/avatar_vader.html#vader-compound-scores",
    "href": "posts/avatar_vader.html#vader-compound-scores",
    "title": "Avatar VADER Sentiment Analysis",
    "section": "VADER compound scores",
    "text": "VADER compound scores\nLuke, I am your Father. You can read more about it vader here: PDF. While it is designed for social media, it also aligns pretty well with my priors about the characters.\n\nStep 1: Sentences. Yip yip!\nI'm going to include my code I used to check my process along the way. Here, I check to make sure get_sentences() is working.\n\navatar_clean %>% head(5) %>% get_sentences() %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nbook\nbook_num\nchapter\nchapter_num\ncharacter\ncharacter_words\nelement_id\nsentence_id\n\n\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nWater.\n1\n1\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nEarth.\n1\n2\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nFire.\n1\n3\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nAir.\n1\n4\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nMy grandmother used to tell me stories about the old days: a time of peace when the Avatar kept balance between the Water Tribes, Earth Kingdom, Fire Nation and Air Nomads.\n1\n5\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nBut that all changed when the Fire Nation attacked.\n1\n6\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nOnly the Avatar mastered all four elements; only he could stop the ruthless firebenders.\n1\n7\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nBut when the world needed him most, he vanished.\n1\n8\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nA hundred years have passed, and the Fire Nation is nearing victory in the war.\n1\n9\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nTwo years ago, my father and the men of my tribe journeyed to the Earth Kingdom to help fight against the Fire Nation, leaving me and my brother to look after our tribe.\n1\n10\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nSome people believe that the Avatar was never reborn into the Air Nomads and that the cycle is broken, but I haven‚Äôt lost hope.\n1\n11\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nI still believe that, somehow, the Avatar will return to save the world.\n1\n12\n\n\n3\nWater\n1\nThe Boy in the Iceberg\n1\nSokka\nIt‚Äôs not getting away from me this time.\n2\n1\n\n\n3\nWater\n1\nThe Boy in the Iceberg\n1\nSokka\nWatch and learn, Katara.\n2\n2\n\n\n3\nWater\n1\nThe Boy in the Iceberg\n1\nSokka\nThis is how you catch a fish.\n2\n3\n\n\n5\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nSokka, look!\n3\n1\n\n\n6\nWater\n1\nThe Boy in the Iceberg\n1\nSokka\nSshh!\n4\n1\n\n\n6\nWater\n1\nThe Boy in the Iceberg\n1\nSokka\nKatara, you‚Äôre going to scare it away.\n4\n2\n\n\n6\nWater\n1\nThe Boy in the Iceberg\n1\nSokka\nMmmm ‚Ä¶\n4\n3\n\n\n6\nWater\n1\nThe Boy in the Iceberg\n1\nSokka\nI can already smell it cookin‚Äô.\n4\n4\n\n\n8\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nBut, Sokka!\n5\n1\n\n\n8\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nI caught one!\n5\n2\n\n\n\n\n\nOkay, it works. Run it.\n\navatar_sentences <- avatar_clean %>% get_sentences()\n\n\n\n\nStep 2: Check structure and test VADER\nCheck the structure of avatar_sentences to setup VADER properly\n\nglimpse(avatar_sentences)\n\nRows: 18,440\nColumns: 9\n$ id              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 5, 6, 6, ~\n$ book            <chr> \"Water\", \"Water\", \"Water\", \"Water\", \"Water\", \"Water\", ~\n$ book_num        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n$ chapter         <chr> \"The Boy in the Iceberg\", \"The Boy in the Iceberg\", \"T~\n$ chapter_num     <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~\n$ character       <chr> \"Katara\", \"Katara\", \"Katara\", \"Katara\", \"Katara\", \"Kat~\n$ character_words <gt_sntnc> \"Water.\", \"Earth.\", \"Fire.\", \"Air.\", \"My grandmot~\n$ element_id      <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 4, 4, ~\n$ sentence_id     <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 1, 1, ~\n\n\nNow for a trial run of vader_df()‚Ä¶ notice we can pull out the the $compound column, I'll use that when I bind the data back into the avatar_sentences dataset.\n\navatar_sentences$character_words %>% head(5) %>% vader_df() %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\ntext\nword_scores\ncompound\npos\nneu\nneg\nbut_count\n\n\n\n\nWater.\n{0}\n0.000\n0.0\n1.000\n0.000\n0\n\n\nEarth.\n{0}\n0.000\n0.0\n1.000\n0.000\n0\n\n\nFire.\n{-1.4}\n-0.340\n0.0\n0.000\n1.000\n0\n\n\nAir.\n{0}\n0.000\n0.0\n1.000\n0.000\n0\n\n\nMy grandmother used to tell me stories about the old days: a time of peace when the Avatar kept balance between the Water Tribes, Earth Kingdom, Fire Nation and Air Nomads.\n{0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2.5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1.4, 0, 0, 0, 0}\n0.273\n0.1\n0.831\n0.069\n0\n\n\n\n\n\n\n\nStep 3: Cross fingers, Run Vader, Make Coffee\nRunning it on all the sentences takes a bit of time. Go make coffee. Check Twitter. Pet the dog.\n\nptm <- proc.time() # Start the clock!\nvader_comp <- avatar_sentences$character_words %>% vader_df() %>% select(compound)\n\nWarning in sentiments[i] <- senti_valence(wpe, i, item): number of items to\nreplace is not a multiple of replacement length\n\nproc.time() - ptm # Calc time\n\n   user  system elapsed \n 101.12   28.54  154.94 \n\n#note, set \"cache=TRUE\" and save some time!\n\nInspect the results\n\n# check it visually\nvader_comp %>% arrange(-compound) %>% head(5)\n\n  compound\n1    0.965\n2    0.944\n3    0.939\n4    0.931\n5    0.927\n\nvader_comp %>% arrange(-compound) %>% tail(5)\n\n      compound\n18436   -0.916\n18437   -0.926\n18438   -0.927\n18439   -0.947\n18440   -0.951\n\n# Any failures?\nvader_comp %>% filter(is.na(compound)) # no NAs\n\n[1] compound\n<0 rows> (or 0-length row.names)\n\nvader_comp %>% filter(is.nan(compound)) # no NaNs\n\n[1] compound\n<0 rows> (or 0-length row.names)\n\n# check dims before binding\ndim(vader_comp)\n\n[1] 18440     1\n\ndim(avatar_sentences)\n\n[1] 18440     9\n\n\n\n\nStep 4: Bind\nTime to add the compound vader score back into our dataset and look at the top 10 instances (again). I could have done this code much more condensed; but, I kept it separated so you follow the process.\n\navatar_sentences %<>% cbind(vader_comp)\navatar_sentences %>% head(5) %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\nbook\nbook_num\nchapter\nchapter_num\ncharacter\ncharacter_words\nelement_id\nsentence_id\ncompound\n\n\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nWater.\n1\n1\n0.000\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nEarth.\n1\n2\n0.000\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nFire.\n1\n3\n-0.340\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nAir.\n1\n4\n0.000\n\n\n1\nWater\n1\nThe Boy in the Iceberg\n1\nKatara\nMy grandmother used to tell me stories about the old days: a time of peace when the Avatar kept balance between the Water Tribes, Earth Kingdom, Fire Nation and Air Nomads.\n1\n5\n0.273\n\n\n\n\n\nLooks good!"
  },
  {
    "objectID": "posts/avatar_vader.html#plot-prep",
    "href": "posts/avatar_vader.html#plot-prep",
    "title": "Avatar VADER Sentiment Analysis",
    "section": "Plot Prep",
    "text": "Plot Prep\nAs I prepare to make some plots there are few things I‚Äôd like to have available:\n\nAn ‚Äúepisode‚Äù number so I can sort and chart books 1-3 because chapter numbers are repeated within each book\nReordered characters - based on their median vader score so that the plots look nice.\n\n\n# create new sequence number for each chapter, because chapter number is repeated within each book\nunique(avatar_sentences$chapter_num)\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21\n\n# add episode_num\navatar_sentences %<>% \n     mutate(episode_num = case_when(\n          book_num == 1 ~ chapter_num,\n          book_num == 2 ~ chapter_num + 21,\n          book_num == 3 ~ chapter_num + 42\n     ))\n\n# add in character vader compound median to sort\navatar_sentences %<>% \n     group_by(character) %>% \n     mutate(char_vader_mean = median(compound[compound != 0])) %>% \n     ungroup() %>% \n     mutate(character = fct_reorder(character, -char_vader_mean))"
  },
  {
    "objectID": "posts/avatar_vader.html#section-1",
    "href": "posts/avatar_vader.html#section-1",
    "title": "Avatar VADER Sentiment Analysis",
    "section": "",
    "text": "Fonts and Themes\nI'm very new to themes and fonts, so not everything worked as well as I'd like. I ended up using some fonts from Google Fonts referenced this excellent blog post by C√©dric Scherer.\n\nlibrary(tvthemes) # great ggplot themes and color palettes\n\nWarning: package 'tvthemes' was built under R version 4.1.3\n\nlibrary(showtext) # use fonts from google\n\nWarning: package 'showtext' was built under R version 4.1.1\n\n\nLoading required package: sysfonts\n\n\nWarning: package 'sysfonts' was built under R version 4.1.1\n\n\nLoading required package: showtextdb\n\nlibrary(extrafont) # use computer fonts\n\nWarning: package 'extrafont' was built under R version 4.1.3\n\n\nRegistering fonts with R\n\n\n\nAttaching package: 'extrafont'\n\n\nThe following object is masked from 'package:showtextdb':\n\n    font_install\n\n#font_import()\nimport_avatar() # import \"Slayer\" font, will need to loadfonts() to access\n\nYou should install these fonts on your system directly. The files are located in [C:/Users/keene/Documents/R/win-library/4.1/tvthemes/fonts/Slayer]\n\nloadfonts(device = \"win\") # Load fonts, can take a minute\n\n# with {showtext}, you can load directly from google fonts\nfont_add_google(\"Indie Flower\", \"Indie Flower\")\n\n# Check the current search path for fonts\n#font_paths() \n\n# List available font files in the search path\n#font_files() \n\n# syntax: font_add(family = \"<family_name>\", regular = \"/path/to/font/file\")\n#font_add(\"Palatino\", \"pala.ttf\")\n\n#font_families()\n\nshowtext_auto()"
  },
  {
    "objectID": "posts/avatar_vader.html#ggplot-bending",
    "href": "posts/avatar_vader.html#ggplot-bending",
    "title": "Avatar VADER Sentiment Analysis",
    "section": "ggplot ‚ÄúBending‚Äù",
    "text": "ggplot ‚ÄúBending‚Äù\nCode for my primary plot, with comments.\nI should also note that I'm still working on developing my plot scaling abilities for rMarkdown, so this code might output the plots with small text for you if you scale up a too high. I often develop and save the plot (the code below), then load the image into the markdown.\n\np1 <- avatar_sentences %>% \n     filter(character %in% c(\"Aang\",\"Sokka\",\"Katara\",\"Zuko\",\"Toph\",\"Iroh\",\"Azula\",\"Jet\",\"Suki\",\"Mai\",\"Ty Lee\")) %>% # only wanted primary characters\n     filter(compound != 0) %>% # pulled out 0 scores, not sure if this is good process or not, but it helps with the viz and tell the story better.\n     ggplot(aes(x = character, y = compound, color = character)) +\n     geom_hline(aes(yintercept = 0), color = \"black\", size = 0.6) + # add line at zero (bc I pull all other lines out in the theme)\n     geom_jitter(position = position_jitter(seed = 2020,width = 0.25), size = 1, alpha = 0.15) + # add a jitter\n     #geom_boxplot(color = \"black\",stat = \"boxplot\",outlier.alpha = 0,fill=NA) + #pulled this out, decided to use the lollipops\n     geom_segment(aes(x = character, xend = character, y = 0, yend = char_vader_mean), size = 1.0,color = \"black\") + # line to zero from median\n     stat_summary(fun = stats::median, geom = \"point\", size = 5, color = \"black\") + # make a slightly bigger dot\n     stat_summary(fun = stats::median, geom = \"point\", size = 4) + # fill it in, I think i could have done this in 1x line with fill/color, but I couldn't figure it out quickly.\n     labs(\n          title = \"Water. Earth. Fire. Air. Vader.\",\n          subtitle = \"Analyzing The Setiment of Avatar's Primary Characters With Compound VADER Scores\",\n          y = \"Compound Vader Scores\",\n          x = NULL,\n          caption = \"Data {appa} by Avery Robbins \\nTidyTuesday 2020-08-11\\n@rbkeeney\"\n          ) +\n     annotate(\"text\", x = 8, y = 1, family = \"Indie Flower\", color = \"gray20\",lineheight = 0.5, size = 6,\n              label = \"Every circle represents a sentence spoken by the character.\\nA higher score correlates to positive sentiment, zero is neutral\"\n              ) +\n     annotate(\"text\", x = 1, y = 0.8, family = \"Indie Flower\", color = \"gray20\", lineheight = 0.5, size = 6,\n              label = \"Ty Lee and Iroh\\nwere the most\\npostive characters\"\n              ) +\n     annotate(\"text\", x = 1, y = -0.4, family = \"Indie Flower\", color = \"gray20\",lineheight = 0.5, size = 6,\n              label = \"All sentences \\nwith a compound \\nscore of 0 have \\nbeen removed\"\n              ) +\n     annotate(\"text\", x = 11, y = -0.8, family = \"Indie Flower\", color = \"gray20\",lineheight = 0.5, size = 6,\n              label = \"Mai was the only\\ncharacter with a \\nnegataive median \\nsentiment\"\n              ) + \n     theme_avatar(\n          title.font = \"Indie Flower\", # wanted to use slayer, but issues getting loaded.\n          text.font = \"Indie Flower\",\n          title.size = 36,\n          subtitle.size = 24\n     ) +\n     theme(\n          axis.title = element_text(size=24),\n          axis.text = element_text(size=24),\n          legend.position = \"none\", # remove legend\n          plot.caption = element_text(size = 16, color = \"grey20\",lineheight = 0.5), #update caption\n          panel.grid.major = element_blank(), # remove plot grids\n          panel.grid.minor = element_blank(), # remove plot grids\n          panel.border = element_rect(colour = \"black\",fill = NA), # box the plot. I like it. Fill = NA ~ no fill.\n          axis.line = element_line(colour = \"black\"),\n          )\n\n# match arrows to annotations... takes time, go arrow by arrow.\np1_arrows <- tibble(\n     x1 = c(1.0, 1.0, 8.5,1,10.6),\n     x2 = c(1.0, 2.0, 8.3,1,10.9),\n     y1 = c(0.67, 0.67, 0.9,-.25,-.65), \n     y2 = c(0.45, 0.38, 0.76,-.05,-.1)\n     )\n\n# combine into final plot\np1_final <- p1 + geom_curve(\n     data = p1_arrows, aes(x = x1, y = y1, xend = x2, yend = y2),\n     arrow = arrow(length = unit(0.07, \"inch\")), size = 0.4,\n     color = \"gray40\", curvature = -0.2\n     )\n\n# To create better pictures for the markdown (1) save it, and (2) then call it in markdown text with: ![alt text here](path-to-image-here)\n# for referecne, this is what roughly outputs on the screen, moving up dpi will shrink the text...\n# ggsave(p1_final, filename = \"figs/Rplot1.png\", dpi = 96, type = \"cairo\", width = 7, height = 5, units = \"in\")\n# dpi: 72-96 for web... 300-400 for high res stuff\n\n# save high res (uncomment)\n#ggsave(p1_final, filename = \"figs/Rplot1.png\", dpi = 300, type = \"cairo\", width = 7, height = 5, units = \"in\")\n\n# open high res, for iteration (comment out once done)\n# img_1 <- magick::image_read('figs/Rplot1.png')\n# print(img_1)\n\n\nFor the second plot, we use facet_wrap() to show how a few characters compound vadar scores over the duration fo the series.\n\np2 <- avatar_sentences %>% \n     filter(character %in% c(\"Aang\",\"Sokka\",\"Katara\",\"Zuko\",\"Toph\",\"Iroh\")) %>% \n     filter(compound != 0) %>% \n     #filter(book == \"Water\") %>% \n     ggplot(aes(x = episode_num, y = compound, color = character)) +\n     #geom_point(alpha = 0.6) +\n     #geom_jitter() +\n     geom_smooth(method = loess, se = F, formula = y ~ x) +\n     facet_wrap(~ character) + \n     coord_cartesian(ylim = c(0,0.3)) +\n     labs(\n          title = \"Water. Earth. Fire. Air. Vader.\",\n          subtitle = \"Analyzing The Change in Setiment Avatar's Primary Characters With Compound VADER Scores Over The Duration Of The Series\",\n          y = \"Compound VADER Scores\",\n          x = \"Nth Episode\",\n          caption = \"Data {appa} by Avery Robbins \\nTidyTuesday 2020-08-11\\n@rbkeeney\"\n          ) +\n     theme_avatar(\n          title.font = \"Indie Flower\",\n          title.size = 36,\n          text.font = \"Indie Flower\",\n          subtitle.size = 24,\n     ) +\n     theme(\n          text = element_text(size = 28, lineheight = 0.5),\n          axis.title = element_text(size=24),\n          axis.text = element_text(size=24),\n          legend.position = \"none\",\n          plot.caption = element_text(color = \"grey20\", size = 16),\n          panel.grid.major = element_blank(), \n          panel.grid.minor = element_blank(),\n          panel.border = element_rect(colour = \"black\",fill = NA),\n          axis.line = element_line(colour = \"black\")\n          )\n\n# uncomment to save\n#ggsave(p2, filename = \"figs/Rplot2.png\", dpi = 300, type = \"cairo\", width = 7, height = 5, units = \"in\") \n# open high res, for iteration (comment out once done)\n#img_2 <- magick::image_read('figs/Rplot2.png')\n#print(img_2)\n\n\nThat wraps it up! Cheers, Ryan"
  },
  {
    "objectID": "posts/ISOMAP_faces.html",
    "href": "posts/ISOMAP_faces.html",
    "title": "Ryan Keeney",
    "section": "",
    "text": "The goal of ISOMAP is to produce a low dimensional representation of the data that preserves the ‚Äòwalking distance‚Äô of the cloud data (manifold).\n\nCreate adjacency matrix of local Euclidean distances: Find neighbors \\(N(i)\\) of each data point, \\(ùë•^i\\), within distance ùúñ and let the adjacency matrix, A, recording neighbor Euclidean distance.\nCreate shortest path pairwise matrix: Matrix D between each pair of points, \\(ùë•^ùëñ\\),\\(ùë•^ùëó\\) based on A.\nReduce dimensional representation: Find a low dimensional representation that preserves the distances in D\n\n\n\n\nimage.png\n\n\nISOMAP is just one example of non-linear dimensional reduction. Other non-linear techniques include local-linear embedding, kernel based methods such as kPCA, manifold learning, and t-SNE. Many of these methods are accessable directly through libraries such as {scikit-learn}.\nThis analysis will break down the ISOMAP Algorithm into individual coded steps.\nISOMAP algorithm: Tenenbaum, J. B., de Silva, V. & Langford, J. C. (2000). A Global Geometric Framework for Nonlinear Dimensionality Reduction. Science, 290, 2319."
  },
  {
    "objectID": "posts/ISOMAP_faces.html#why-not-pca-or-svd",
    "href": "posts/ISOMAP_faces.html#why-not-pca-or-svd",
    "title": "Ryan Keeney",
    "section": "Why not PCA or SVD?",
    "text": "Why not PCA or SVD?\nPCA or SVD are suitable when variables are linearly correlated, as they use linear projection \\(~w^T (x^i‚àíŒº)\\), implicitly assuming Euclidean distance is the dissimilarity (distance) measure.\nWhen there is a nonlinear structure, Euclidean distance is not the right distance measure globally. Locally, Euclidean distance is okay!"
  },
  {
    "objectID": "posts/ISOMAP_faces.html#isomap-algorithm",
    "href": "posts/ISOMAP_faces.html#isomap-algorithm",
    "title": "Ryan Keeney",
    "section": "ISOMAP Algorithm",
    "text": "ISOMAP Algorithm\n\nIdentify local neighbors using a weighted nearest neighbor graph\n\nœµ-ISOMAP: Build connections if the distance is closer than epslion.\nK-ISOMAP: Select k nearest neighbors (better for sparse data)\n\nCompute pairwise shortest distance matrix D\n\nFloyd-Warshall algorithm\nDijkstra‚Äôs algorithm\n\nCompute The centering matrix, H, via MDS (Multi-dimensional scaling). Essentially, given Euclidean distances, compute the resulting coordinates.\nReduce dimensionality of D through eigen decomposition"
  },
  {
    "objectID": "posts/ISOMAP_faces.html#libraries",
    "href": "posts/ISOMAP_faces.html#libraries",
    "title": "Ryan Keeney",
    "section": "Libraries",
    "text": "Libraries\n\n# Import Libraries\nimport math\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.io as sio\n\nfrom sklearn.neighbors import radius_neighbors_graph\nfrom sklearn.neighbors import kneighbors_graph\nfrom scipy.sparse.csgraph import shortest_path\nimport scipy.sparse.linalg as ll"
  },
  {
    "objectID": "posts/ISOMAP_faces.html#load-and-prep-data",
    "href": "posts/ISOMAP_faces.html#load-and-prep-data",
    "title": "Ryan Keeney",
    "section": "Load and prep data",
    "text": "Load and prep data\n\n# load mat file\ndata_isomap = sio.loadmat('data/isomap.mat', squeeze_me=True)\n\n# set up array of images\nimages = data_isomap['images'].T\nn,m = images.shape\nprint(f'n:{n}, m:{m}, type: {type(images)}')\n\nn:698, m:4096, type: <class 'numpy.ndarray'>"
  },
  {
    "objectID": "posts/ISOMAP_faces.html#nearest-neighbor-graph-a",
    "href": "posts/ISOMAP_faces.html#nearest-neighbor-graph-a",
    "title": "Ryan Keeney",
    "section": "Nearest neighbor graph, A",
    "text": "Nearest neighbor graph, A\nNotes on tuning ùúñ\nEpsilon was tuned using + 0.1 increments until the pairwise shortest path matrix, D, held no infinite values. I compared this to the k-nearest neighbor graphs of recommended size ùëò=5 (ISOMAP default). Since the selected setting ùúñ‚â•10.5, was much denser than the default k-nearest neighbor graph, I selected the minimum value that fully populated the D matrix. The goal was to avoid ‚Äòshort-circuit‚Äô errors when k or epsilon are too large, or errors that result when the matrix is too sparse when k is too small.\nYou may find it easier to use {kneighbors_graph} with the recommended ISOMAP neighbors of 8<=k<=12 per M. Balasubramanian, E. L. Schwartz, The Isomap Algorithm and Topological Stability. Science 4 January 2002: Vol. 295 no. 5552 p.¬†7.\n\n# set epsilon or k\n# 10.5 min, or else there are some non-connections and the code will break! \n# Note this is much more dense that the recommended ISOMAP neighbor setting\nepsilon = 10.5 \nneighbors = 5\n\n# create nearest neighbor graph\nA = radius_neighbors_graph(\n    X = images, \n    radius = epsilon,\n    mode='distance',\n    metric='minkowski',\n    p=2,\n    include_self=True,\n    n_jobs = -1)\n\nprint(f'A: {A.shape}')\n\n# alternative, using recommendation for n_neighbors for ISOMAP\nA_knn = kneighbors_graph(\n    X = images, \n    n_neighbors = neighbors,\n    mode='distance',\n    metric='minkowski',\n    p=2, \n    include_self=False,\n    n_jobs = -1)\n\n# plot\nplt.figure(figsize=(7, 7))\nplt.title(\"radius_neighbors_graph\")\nplt.spy(A,markersize=2)\nplt.show()\n\n# plot\nplt.figure(figsize=(7, 7))\nplt.title(\"kneighbors_graph\")\nplt.spy(A_knn,markersize=2)\nplt.show()\n\nA: (698, 698)\n\n\n\n\n\n\n\n\n\n# plot distances\n# print(A)\n# (697, 374) 2.27\n# (0, 149) 5.58\n# (0, 574) 10.15\n\n# plot a figure\nplt.figure(figsize = (3,3))\nimg = images[574, :].reshape(64, 64).T\nplt.imshow(img, aspect='auto', interpolation='nearest',cmap=plt.cm.gray,zorder=1)\n\n<matplotlib.image.AxesImage at 0x191b517ce20>"
  },
  {
    "objectID": "posts/ISOMAP_faces.html#shortest-path-gaph-d",
    "href": "posts/ISOMAP_faces.html#shortest-path-gaph-d",
    "title": "Ryan Keeney",
    "section": "Shortest path gaph, D",
    "text": "Shortest path gaph, D\n\nD = shortest_path(\n    csgraph= A,\n    method = 'FW',\n    directed=False)\n\nprint(f'D: {D.shape}, type: {type(D)}')\n\n# count infinites...\nprint(f'Number of inf cases: {np.sum(D ==np.inf)}')\n\nD: (698, 698), type: <class 'numpy.ndarray'>\nNumber of inf cases: 0"
  },
  {
    "objectID": "posts/ISOMAP_faces.html#eigen-decomposition",
    "href": "posts/ISOMAP_faces.html#eigen-decomposition",
    "title": "Ryan Keeney",
    "section": "Eigen decomposition",
    "text": "Eigen decomposition\n\n# create centering matrix\nI = np.identity(n)\nones = np.ones((n, n))\nH = I - (1 / n)*(ones)\n\n# calc D^2 matrix\nD_sq = D**2\n\n# G matrix\nG = (-1 /2) * (H.dot(D_sq).dot(H))\n\n# get eigens\nS,W = ll.eigs(G,k = 2)\nS = S.real #eigenvalues\nW = W.real #eigenvectors\n\n# dim1 and dim2\ndim1 = W[:,0] * math.sqrt(S[0])\ndim2 = W[:,1] * math.sqrt(S[1])"
  },
  {
    "objectID": "posts/ISOMAP_faces.html#plot",
    "href": "posts/ISOMAP_faces.html#plot",
    "title": "Ryan Keeney",
    "section": "Plot",
    "text": "Plot\n\nfigure = plt.figure()\nfigure.set_size_inches(10, 10)\nax1 = figure.add_subplot(111)\n\n# title and axis labels\nax1.set_title(\"ISOMAP\")\nax1.set_xlabel('dim1')\nax1.set_ylabel('dim2')\n\n# axis ranges\nx_size = (max(dim1) - min(dim1)) * 0.06\ny_size = (max(dim2) - min(dim2)) * 0.06\n\n# add 30 random images\nfor i in range(30):\n    # get random photo\n    img_num = np.random.randint(0, n)\n    # starting and ending x, y for image\n    x0 = dim1[img_num] - (x_size / 2.)\n    y0 = dim2[img_num] - (y_size / 2.)\n    x1 = dim1[img_num] + (x_size / 2.)\n    y1 = dim2[img_num] + (y_size / 2.)\n    img = images[img_num, :].reshape(64, 64).T\n    ax1.imshow(img, aspect='auto', interpolation='nearest',cmap=plt.cm.gray, zorder=1, extent=(x0, x1, y0, y1))\n\n# add dots\nax1.scatter(dim1, dim2, marker='.',c='blue', alpha=1)\n\n# show\nplt.show()\n\n\n\n\n\n# load mat file\ndata_isomap = sio.loadmat('data/isomap.mat', squeeze_me=True)\n\n# set up array of images\nimages = data_isomap['images'].T\nn,m = images.shape\nprint(f'n:{n}, m:{m}, type: {type(images)}')\n\nn:698, m:4096, type: <class 'numpy.ndarray'>"
  },
  {
    "objectID": "posts/ISOMAP_faces.html#pca",
    "href": "posts/ISOMAP_faces.html#pca",
    "title": "Ryan Keeney",
    "section": "PCA",
    "text": "PCA\n\n%reset -f\n\n\n# Import Libraries\nimport math\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.io as sio\n\nfrom sklearn.neighbors import radius_neighbors_graph\nfrom sklearn.neighbors import kneighbors_graph\nfrom scipy.sparse.csgraph import shortest_path\nimport scipy.sparse.linalg as ll\n\n\n# load mat file\ndata_isomap = sio.loadmat('data/isomap.mat', squeeze_me=True)\n\n# set up array of images\nimages = data_isomap['images'].T\nn,m = images.shape\nprint(f'n:{n}, m:{m}, type: {type(images)}')\n\nn:698, m:4096, type: <class 'numpy.ndarray'>\n\n\n\nAnew = images.T\n\nmu = np.mean(Anew,axis = 1)\nxc = Anew - mu[:,None]\n\nC = np.dot(xc,xc.T)/m\n\nS,W = ll.eigs(C,k = 2)\nS = S.real\nW = W.real\n\ndim1 = np.dot(W[:,0].T,xc)/math.sqrt(S[0]) # extract 1st eigenvalues\ndim2 = np.dot(W[:,1].T,xc)/math.sqrt(S[1]) # extract 2nd eigenvalue\n\nfigure2 = plt.figure()\nfigure2.set_size_inches(10, 10)\nax2 = figure2.add_subplot(111)\n\n# title and axis labels\nax2.set_title(\"PCA\")\nax2.set_xlabel('dim1')\nax2.set_ylabel('dim2')\n\n# axis ranges\nx_size = (max(dim1) - min(dim1)) * 0.06\ny_size = (max(dim2) - min(dim2)) * 0.06\n\n# add 30 random images\nfor i in range(30):\n    # get random photo\n    img_num = np.random.randint(0, n)\n    # starting and ending x, y for image\n    x0 = dim1[img_num] - (x_size / 2.)\n    y0 = dim2[img_num] - (y_size / 2.)\n    x1 = dim1[img_num] + (x_size / 2.)\n    y1 = dim2[img_num] + (y_size / 2.)\n    img = images[img_num, :].reshape(64, 64).T\n    ax2.imshow(img, aspect='auto', interpolation='nearest',cmap=plt.cm.gray, zorder=1, extent=(x0, x1, y0, y1))\n\n# add dots\nax2.scatter(dim1, dim2, marker='.',c='blue', alpha=1)\n\n# show\nplt.show()"
  },
  {
    "objectID": "posts/ISOMAP_faces.html#results",
    "href": "posts/ISOMAP_faces.html#results",
    "title": "Ryan Keeney",
    "section": "Results",
    "text": "Results\nISOMAP organizes the images in a more meaningful way than PCA. For example, clusters of like images are facing to the same direction from left to right, and top to bottom. The amount of rotation is also captured along the 2 dimensions. Dim 2 seems to capture up-down pose and dim 1 capture left-right pose.\nIn PCA, images are organized in a less meaningful way than ISOMAP. It seems that dim 1 captures darker-to-lighter changes while dim2 captures light vs.¬†dark for the right-to-left side of the image."
  },
  {
    "objectID": "posts/missing_data.html",
    "href": "posts/missing_data.html",
    "title": "Missing Data",
    "section": "",
    "text": "In this post, we will review types of missing data and methods for dealing with missing data. Data is often missing, wrong, and generally just has a lot of problems.\n\nMeasurement gauges break down\nRecording errors occur during transmission, data entry, and storage.\nHuman interaction causes a variety of challenges such as incorrect inputs, withheld or unavailable data, or medical history.\n\n\nThis post does not deal with censored data. Censored data occurs when an data point has not yet been observed. For example, if you are studying a components reliability, it may be unrealistic to run all the components to failure. If a test is suspended without the component failing, then it‚Äôs actual failure point is censored - you only know the lower-bound of the components life-time. Bayesian approaches are particularly effective when censored data occurs in time-to-event applications such as reliability theory, survival theory (healthcare outcomes), and geoscientific predictions (when will Mount Rainier erupt next?).\n\n\n\nMissing Completely At Random (MCAR): Missingness does not depend on observed or unobserved data. There is no systematic differences between what is observed and what is not.\n\nEasy to deal with\nIgnorable missingness\n\nMissing At Random (MAR): Missingness depends only on the observed data. MAR occurs when the missingness is not random - there are systematic differences between what is observed and what is not - but where missingness can be fully accounted for by variables where there is complete information.\n\nEasy to deal with\nIgnorable missingness\n\nThe missing data correlated to other data in the data set. For example, perhaps it‚Äôs dangerous (to us) to measure the bills of large and aggressive penguins, so that variable may be missing or inaccurately measured for those types of penguins.\nMissing Not At Random (MNAR): Neither MCAR, nor MAR hold; missingness may depend on the data that is missing - there are systematic differences between what is observed and what is not - and the causes are not accounted for. Usually, MNAR indicates that the situations at which missingness occurs depends on hidden or unobserved causes. This is the most dangerous and difficult type of missingness.\n\nDifficult to deal with\nNon-ignorable missingness\n\nWhen the data is Missing Not At Random (MNAR), which missing values may depend on other factors such as data collection design, reporting biases, selection biases.\n\n\n\nSome factors that are more likely to be missing (e.g.¬†due to difficulty of collection, or isn‚Äôt standard to be collected).\nOther factor types are more likely to be missing as well for more complex reasons (e.g.¬†bias for or not providing income levels, a radar gun used for speeds outside its operating range, death date won‚Äôt be recorded for living patients). This results in bias, and must be accounted for differently."
  },
  {
    "objectID": "posts/missing_data.html#omit-missing-data",
    "href": "posts/missing_data.html#omit-missing-data",
    "title": "Missing Data",
    "section": "Omit missing data",
    "text": "Omit missing data\nThe first option is to simply omit or discard the missing data. That‚Äôs easy to implement and doesn‚Äôt potentially induce errors. However, you have to weight this against the risk of losing too many data points - many large data have hundreds or thousands of variables, if you removed any data point with a missing variables you could eliminate practically the entire data. Additionally, simply removing data creates the potential for for censored or biased missing data.\n\nExample: Omit\n\nrec_omit <- rec_base |> \n  step_naomit(all_predictors()) |> \n  prep(data_train_missing)\n\n# apply to missing data\ndata_omit <- bake(rec_omit, new_data=NULL) \n\n# we lose about 100 data points if we choose to omit all the missing data!\ndata_omit |> nrow()\n\n[1] 145\n\ndata_train_full |> nrow()\n\n[1] 249"
  },
  {
    "objectID": "posts/missing_data.html#use-categorical-variables-to-indicate-missing-data",
    "href": "posts/missing_data.html#use-categorical-variables-to-indicate-missing-data",
    "title": "Missing Data",
    "section": "Use categorical variables to indicate missing data",
    "text": "Use categorical variables to indicate missing data\nThe missing data can be biased! To account for that we can include interactions.\nIf we include interaction terms between the new categorical variable and all other variables, then essentially we‚Äôre creating two separate models. One for when there‚Äôs missing data in this variable and one for when there isn‚Äôt. So it‚Äôs really like a tree model with a single branch.\n\nExample: Categorical value\nHere, I build a categorical missing value for island.\n\nrec_cat <- rec_base |> \n  # convert to character (easier)\n  step_mutate(island = as.character(island)) |>\n  # Change NA -> \"Missing\"\n  step_mutate(island = ifelse(is.na(island),'Missing',island)) |>\n  # covert back to factor\n  step_mutate(island = as.factor(island)) |>\n  # dummy step with one hot encoding\n  step_dummy(island,one_hot = TRUE) |> \n  # set interaction term between the missing island category and all other vars\n  step_interact(terms = ~island_Missing:all_predictors()) |> \n  # train\n  prep(data_train_missing)\n\n# apply to missing data\ndata_cat <- bake(rec_cat, new_data=NULL)\n\n# these are the new variables for the model\ndata_cat |> names()\n\n [1] \"bill_length_mm\"                     \"bill_depth_mm\"                     \n [3] \"flipper_length_mm\"                  \"body_mass_g\"                       \n [5] \"sex\"                                \"species\"                           \n [7] \"island_Biscoe\"                      \"island_Dream\"                      \n [9] \"island_Missing\"                     \"island_Torgersen\"                  \n[11] \"island_Missing_x_bill_length_mm\"    \"island_Missing_x_bill_depth_mm\"    \n[13] \"island_Missing_x_flipper_length_mm\" \"island_Missing_x_body_mass_g\"      \n[15] \"island_Missing_x_sexmale\"           \"island_Missing_x_island_Biscoe\"    \n[17] \"island_Missing_x_island_Dream\"      \"island_Missing_x_island_Torgersen\" \n\n\n\n\nExample: Categorical value for numeric data\nFor numerical values, set NA = 0 and then add in a missing column. In this example, I create a missing term for bill length, then create the required interaction terms.\n\nrec_cat_num <- rec_base |> \n  # set up missing category\n  step_mutate(bill_length_missing = ifelse(is.na(bill_length_mm),'Yes','No')) |>\n  # set bill length -> 0 if NA\n  step_mutate(bill_length_mm = ifelse(is.na(bill_length_mm),0,bill_length_mm)) |>\n  # convert the missing category to a factor\n  step_mutate(bill_length_missing = as.factor(bill_length_missing)) |> \n  # dummy step with one hot encoding\n  step_dummy(bill_length_missing,one_hot = TRUE) |> \n  # set interaction term between the missing  category and all other vars\n  step_interact(terms = ~bill_length_missing_Yes:all_predictors()) |> \n  # train\n  prep(data_train_missing)\n\n# apply to missing data\ndata_cat_num <- bake(rec_cat_num, new_data=NULL)\n\n# these are the new variables for the model\ndata_cat_num |> names()\n\n [1] \"island\"                                          \n [2] \"bill_length_mm\"                                  \n [3] \"bill_depth_mm\"                                   \n [4] \"flipper_length_mm\"                               \n [5] \"body_mass_g\"                                     \n [6] \"sex\"                                             \n [7] \"species\"                                         \n [8] \"bill_length_missing_No\"                          \n [9] \"bill_length_missing_Yes\"                         \n[10] \"bill_length_missing_Yes_x_islandDream\"           \n[11] \"bill_length_missing_Yes_x_islandTorgersen\"       \n[12] \"bill_length_missing_Yes_x_bill_length_mm\"        \n[13] \"bill_length_missing_Yes_x_bill_depth_mm\"         \n[14] \"bill_length_missing_Yes_x_flipper_length_mm\"     \n[15] \"bill_length_missing_Yes_x_body_mass_g\"           \n[16] \"bill_length_missing_Yes_x_sexmale\"               \n[17] \"bill_length_missing_Yes_x_bill_length_missing_No\""
  },
  {
    "objectID": "posts/missing_data.html#estimate-missing-data",
    "href": "posts/missing_data.html#estimate-missing-data",
    "title": "Missing Data",
    "section": "Estimate missing data",
    "text": "Estimate missing data\n\nGeneral guidelines for imputation\n\nData is used twice, so it can lead to over-fitting\nLimit the amount of imputation to no more than 5% per factor\nIf more than 5% is missing, use omission or categorical value methods\n\n\n\nApproaches to imputation\n\nMid-range value: use mean, median (numeric), or mode (categorical)\nRegression: Reduce or eliminate the problem of bias by using other factors to predict the missing value. Essentially, build a model for each factor.\nPerturbation: Accounts for bias and variability. Essentially, add perturbation to each imputed variable (e.g.¬†adjust up/down a random amount from the normally distributed variation).\n\n\n\n\n\n\n\n\n\nMethod\nPro\nCon\n\n\n\n\nMid-range value\n\nHedge against being too wrong\nEasy to compute\n\n\nBiased imputation (e.g.¬†if high-income people are less likely to answer the mean/median will be underestimate the value)\n\n\n\nRegression\n\nReduce or eliminate the problem of bias\n\n\nComplex to build, fit, validate, and test additional models\nDoes not capture all the variability\nUses the data twice, so it could over-fit\n\n\n\nPerturbation\n\nMore accurate variability\n\n\nLess accurate on average\n\n\n\n\nDo we add additional error from imputation and perturbation?\nYup! Total error = Imputation error + perturbation error + model error. However, regular data almost certainly has errors as well. It‚Äôs up to you as the data scientist to decide what trade-offs to make in a given situation.\nThere are many approaches to imputation. For example, advanced methods like multivariate imputation by chained equations (MICE) can impute multiple factor values together.\n\n\nExample: Estimation (Imputation)\nLet‚Äôs estimate a few of the numerical values with different methods. Permutation sampling is covered in rsample.\n\nrec_impute <- rec_base |> \n  # impute bill length and depth with mean\n  step_impute_mean(bill_length_mm,bill_depth_mm,flipper_length_mm) |> \n  # impute sex with mode\n  step_impute_mode(sex) |> \n  # impute body mass with linear model mass ~ sex + bill_length_mm +bill_depth_mm\n  step_impute_linear(body_mass_g, impute_with = imp_vars(sex,bill_length_mm,bill_depth_mm)) |> \n  # impute island with knn\n  step_impute_knn(island, neighbors = 5) |> \n  # train\n  prep(data_train_missing)\n  \n# apply to missing data\ndata_impute <- bake(rec_impute, new_data=NULL)\n\n# check\ncolSums(is.na(data_impute))\n\n           island    bill_length_mm     bill_depth_mm flipper_length_mm \n                0                 0                 0                 0 \n      body_mass_g               sex           species \n                0                 0                 0"
  },
  {
    "objectID": "posts/PCA_eu_food.html",
    "href": "posts/PCA_eu_food.html",
    "title": "Ryan Keeney",
    "section": "",
    "text": "PCA is unsupervised learning (contains no label information). It is usually used to explore data and understand patterns and used to set up clustering analysis.\nObjective of clustering is to find patterns and relationships within a dataset, such as: - Geometric (feature distances) - Connectivity (spectral clustering, graphs)\nHowever, the original dimensionally {R^n} can be large! The goal is to reduce it, k‚â™n!\nDimensionality reduction is the process of reducing the number of random variables under consideration - Combine, transform or select variables - Can use linear or nonlinear operations\nPCA, or prinicpal component analysis, is one common way to perform dimensionality reduction. Dimensionality reduction has many uses in data science: - Visualizing, exploring and understanding the data - Extracting ‚Äùfeatures‚Äù and dominant modes - Cleaning data - Speeding up subsequent learning task - Building simpler model later\nIt has many practical applications as well, such as image/audio compression, face recognition, and natural language processing (latent semantic analysis) to name a few!"
  },
  {
    "objectID": "posts/PCA_eu_food.html#pca-by-hand",
    "href": "posts/PCA_eu_food.html#pca-by-hand",
    "title": "Ryan Keeney",
    "section": "PCA, by hand",
    "text": "PCA, by hand\nStep 1: Normalize\nNormalize (sometimes called scaling and/or standardizing) your data, especially if the different dimensions contain different scales of data.\nStep 2: Estimate the mean and covariance matrix from the data\nThe covariance matrix, C, captures the variability of the data points along different directions, it also captures correlations, covariance between different coordinates (features in the x vector).\nStep 3: Take the largest K eigenvectors of C which correspond to the largest eigenvalues\nUse a solver for this part, such as {eigs} function from scipy.sparse.linalg. In this case, I used K=2 to reduce the data to 2 dimensions.\nC=UŒõU^T\nC has d√ód dimensions\nU=eigenvectors\nStep 4: Compute the reduced representation of a data point\nKey Concept: PCA is a form of linear reduction scheme, because the prinicpal component \\(z_i\\) is linear transformation of the x."
  },
  {
    "objectID": "posts/PCA_eu_food.html#food-consumption-in-european-countries",
    "href": "posts/PCA_eu_food.html#food-consumption-in-european-countries",
    "title": "Ryan Keeney",
    "section": "Food consumption in European countries",
    "text": "Food consumption in European countries\nThis anaylsis uses PCA to compare European countries consumption of various food and drink."
  },
  {
    "objectID": "posts/PCA_eu_food.html#libraries",
    "href": "posts/PCA_eu_food.html#libraries",
    "title": "Ryan Keeney",
    "section": "Libraries",
    "text": "Libraries\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport scipy.io as spio\nimport scipy.sparse.linalg as ll\nimport sklearn.preprocessing as skpp"
  },
  {
    "objectID": "posts/PCA_eu_food.html#load-and-prep-data",
    "href": "posts/PCA_eu_food.html#load-and-prep-data",
    "title": "Ryan Keeney",
    "section": "Load and Prep Data",
    "text": "Load and Prep Data\n\n# Import CSV and save it in different dataframes\nfood_consumption = pd.read_csv(\"data/food-consumption.csv\")\ndisplay(food_consumption)\n\n# pull out country list\ncountries = food_consumption['Country']\n#display(countries)\n\n# remove country column, convert to array\nAnew = food_consumption.drop(columns=['Country']).to_numpy()\n#display(Anew)\n\n# get shape of matrix\nm, n = Anew.shape\n\n\n\n\n\n  \n    \n      \n      Country\n      Real coffee\n      Instant coffee\n      Tea\n      Sweetener\n      Biscuits\n      Powder soup\n      Tin soup\n      Potatoes\n      Frozen fish\n      ...\n      Apples\n      Oranges\n      Tinned fruit\n      Jam\n      Garlic\n      Butter\n      Margarine\n      Olive oil\n      Yoghurt\n      Crisp bread\n    \n  \n  \n    \n      0\n      Germany\n      90\n      49\n      88\n      19\n      57\n      51\n      19\n      21\n      27\n      ...\n      81\n      75\n      44\n      71\n      22\n      91\n      85\n      74\n      30\n      26\n    \n    \n      1\n      Italy\n      82\n      10\n      60\n      2\n      55\n      41\n      3\n      2\n      4\n      ...\n      67\n      71\n      9\n      46\n      80\n      66\n      24\n      94\n      5\n      18\n    \n    \n      2\n      France\n      88\n      42\n      63\n      4\n      76\n      53\n      11\n      23\n      11\n      ...\n      87\n      84\n      40\n      45\n      88\n      94\n      47\n      36\n      57\n      3\n    \n    \n      3\n      Holland\n      96\n      62\n      98\n      32\n      62\n      67\n      43\n      7\n      14\n      ...\n      83\n      89\n      61\n      81\n      15\n      31\n      97\n      13\n      53\n      15\n    \n    \n      4\n      Belgium\n      94\n      38\n      48\n      11\n      74\n      37\n      23\n      9\n      13\n      ...\n      76\n      76\n      42\n      57\n      29\n      84\n      80\n      83\n      20\n      5\n    \n    \n      5\n      Luxembourg\n      97\n      61\n      86\n      28\n      79\n      73\n      12\n      7\n      26\n      ...\n      85\n      94\n      83\n      20\n      91\n      94\n      94\n      84\n      31\n      24\n    \n    \n      6\n      England\n      27\n      86\n      99\n      22\n      91\n      55\n      76\n      17\n      20\n      ...\n      76\n      68\n      89\n      91\n      11\n      95\n      94\n      57\n      11\n      28\n    \n    \n      7\n      Portugal\n      72\n      26\n      77\n      2\n      22\n      34\n      1\n      5\n      20\n      ...\n      22\n      51\n      8\n      16\n      89\n      65\n      78\n      92\n      6\n      9\n    \n    \n      8\n      Austria\n      55\n      31\n      61\n      15\n      29\n      33\n      1\n      5\n      15\n      ...\n      49\n      42\n      14\n      41\n      51\n      51\n      72\n      28\n      13\n      11\n    \n    \n      9\n      Switzerland\n      73\n      72\n      85\n      25\n      31\n      69\n      10\n      17\n      19\n      ...\n      79\n      70\n      46\n      61\n      64\n      82\n      48\n      61\n      48\n      30\n    \n    \n      10\n      Sweden\n      97\n      13\n      93\n      31\n      61\n      43\n      43\n      39\n      54\n      ...\n      56\n      78\n      53\n      75\n      9\n      68\n      32\n      48\n      2\n      93\n    \n    \n      11\n      Denmark\n      96\n      17\n      92\n      35\n      66\n      32\n      17\n      11\n      51\n      ...\n      81\n      72\n      50\n      64\n      11\n      92\n      91\n      30\n      11\n      34\n    \n    \n      12\n      Norway\n      92\n      17\n      83\n      13\n      62\n      51\n      4\n      17\n      30\n      ...\n      61\n      72\n      34\n      51\n      11\n      63\n      94\n      28\n      2\n      62\n    \n    \n      13\n      Finland\n      98\n      12\n      84\n      20\n      64\n      27\n      10\n      8\n      18\n      ...\n      50\n      57\n      22\n      37\n      15\n      96\n      94\n      17\n      21\n      64\n    \n    \n      14\n      Spain\n      70\n      40\n      40\n      18\n      62\n      43\n      2\n      14\n      23\n      ...\n      59\n      77\n      30\n      38\n      86\n      44\n      51\n      91\n      16\n      13\n    \n    \n      15\n      Ireland\n      30\n      52\n      99\n      11\n      80\n      75\n      18\n      2\n      5\n      ...\n      57\n      52\n      46\n      89\n      5\n      97\n      25\n      31\n      3\n      9\n    \n  \n\n16 rows √ó 21 columns"
  },
  {
    "objectID": "posts/PCA_eu_food.html#normalize-data",
    "href": "posts/PCA_eu_food.html#normalize-data",
    "title": "Ryan Keeney",
    "section": "Normalize data",
    "text": "Normalize data\nIn this case, we normalize the data because features have very different ranges\n\nstdA = np.std(Anew,axis = 0)\nAnew = Anew @ np.diag(np.ones(stdA.shape[0])/stdA)\nAnew = Anew.T\n\n#display(pd.DataFrame(Anew))"
  },
  {
    "objectID": "posts/PCA_eu_food.html#pca",
    "href": "posts/PCA_eu_food.html#pca",
    "title": "Ryan Keeney",
    "section": "PCA",
    "text": "PCA\nHere, we extract the first two dimensions following the steps listed above.\n\nmu = np.mean(Anew,axis = 1)\nxc = Anew - mu[:,None]\n\nC = np.dot(xc,xc.T)/m\n\nK=2\nS,W = ll.eigs(C,k = K)\nS = S.real\nW = W.real\n\ndim1 = np.dot(W[:,0].T,xc)/math.sqrt(S[0]) # extract 1st eigenvalues\ndim2 = np.dot(W[:,1].T,xc)/math.sqrt(S[1]) # extract 2nd eigenvalue"
  },
  {
    "objectID": "posts/PCA_eu_food.html#plot-1",
    "href": "posts/PCA_eu_food.html#plot-1",
    "title": "Ryan Keeney",
    "section": "Plot 1",
    "text": "Plot 1\nThe location of the countries does seem logical (countries close to one another are likely to consume similar foods) ‚Äì Nordic countries are grouped together, as are countries in the S-SW (Portugal, Spain, Italy) with a likely strong Mediterranean influence. Additionally, countries in central Europe are grouped (Germany, Belgium, Switzerland).\n\nplt.figure(figsize=(7, 7))\nplt.xlabel(\"dim1\")\nplt.ylabel(\"dim2\")\nplt.plot(dim1, dim2, 'r.')\nplt.axvline(0, linewidth=0.5)\nplt.axhline(0, linewidth=0.5)\n\n# add labels\nfor i in range(0, countries.shape[0]):\n    plt.text(dim1[i], dim2[i] + 0.01, countries[i])\nplt.show()"
  },
  {
    "objectID": "posts/PCA_eu_food.html#check-correlations-to-dim1-and-dim2",
    "href": "posts/PCA_eu_food.html#check-correlations-to-dim1-and-dim2",
    "title": "Ryan Keeney",
    "section": "Check correlations to dim1 and dim2",
    "text": "Check correlations to dim1 and dim2\nDimension 1 (PC 1) is highly correlated to countries with high consumption of tea, sweetener, tin soup, frozen veggies, and tinned fruit. Dimension 2 is highly correlated with high consumption of instant coffee, powder soup, and low consumption of frozen fish and crisp bread).\n\n# add dim1 and dim2\nfood_consumption['dim1'] = dim1.tolist()\nfood_consumption['dim2'] = dim2.tolist()\ndisplay(food_consumption.drop(columns=['Country']).corr(method='pearson').tail(2).style.format('{0:,.2f}'))\n\n\n                    Real coffee        Instant coffee        Tea        Sweetener        Biscuits        Powder soup        Tin soup        Potatoes        Frozen fish        Frozen veggies        Apples        Oranges        Tinned fruit        Jam        Garlic        Butter        Margarine        Olive oil        Yoghurt        Crisp bread        dim1        dim2    \n                \n                        dim1\n                        0.08\n                        0.43\n                        0.70\n                        0.78\n                        0.58\n                        0.43\n                        0.77\n                        0.51\n                        0.51\n                        0.75\n                        0.61\n                        0.51\n                        0.88\n                        0.68\n                        -0.61\n                        0.29\n                        0.30\n                        -0.38\n                        0.21\n                        0.43\n                        1.00\n                        0.00\n            \n            \n                        dim2\n                        -0.39\n                        0.78\n                        -0.09\n                        -0.23\n                        0.30\n                        0.68\n                        0.13\n                        -0.36\n                        -0.72\n                        -0.54\n                        0.48\n                        0.26\n                        0.35\n                        0.13\n                        0.36\n                        0.13\n                        -0.08\n                        0.21\n                        0.55\n                        -0.76\n                        0.00\n                        1.00"
  },
  {
    "objectID": "posts/PCA_eu_food.html#flipping-the-analysis",
    "href": "posts/PCA_eu_food.html#flipping-the-analysis",
    "title": "Ryan Keeney",
    "section": "Flipping the Analysis",
    "text": "Flipping the Analysis\nIn the analysis above, we looked at which countries were similar to one another based on their food consumptions. In the next set of analysis, we will look at how foods are related to another based on the countries were they are eaten.\nIf part 1 was comparing culinary culures by country, this next analysis can be thought of as comparing how common (or uncommon) food pairings are.\nFirst, let‚Äôs treat this as a completely new problem and reset our notebook.\n\n%reset -f"
  },
  {
    "objectID": "posts/PCA_eu_food.html#libraries-1",
    "href": "posts/PCA_eu_food.html#libraries-1",
    "title": "Ryan Keeney",
    "section": "Libraries",
    "text": "Libraries\n\nimport numpy as np\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport scipy.io as spio\nimport scipy.sparse.linalg as ll\nimport sklearn.preprocessing as skpp"
  },
  {
    "objectID": "posts/PCA_eu_food.html#load-and-prep-data-1",
    "href": "posts/PCA_eu_food.html#load-and-prep-data-1",
    "title": "Ryan Keeney",
    "section": "Load and Prep Data",
    "text": "Load and Prep Data\n\n# Import CSV and save it in different dataframes\nfood_consumption = pd.read_csv(\"data/food-consumption.csv\")\n\n# transpose df\nfood_consumption = food_consumption.set_index('Country').T.rename_axis(index='Food Item', columns=None).reset_index()\ndisplay(food_consumption)\n\n# pull out country list\nfood_items = food_consumption['Food Item']\n#display(food_items)\n\n# remove food item column, convert to array\nAnew = food_consumption.drop(columns=['Food Item']).to_numpy()\n#display(Anew)\n\n# get shape of matrix\nm, n = Anew.shape\n\n\n\n\n\n  \n    \n      \n      Food Item\n      Germany\n      Italy\n      France\n      Holland\n      Belgium\n      Luxembourg\n      England\n      Portugal\n      Austria\n      Switzerland\n      Sweden\n      Denmark\n      Norway\n      Finland\n      Spain\n      Ireland\n    \n  \n  \n    \n      0\n      Real coffee\n      90\n      82\n      88\n      96\n      94\n      97\n      27\n      72\n      55\n      73\n      97\n      96\n      92\n      98\n      70\n      30\n    \n    \n      1\n      Instant coffee\n      49\n      10\n      42\n      62\n      38\n      61\n      86\n      26\n      31\n      72\n      13\n      17\n      17\n      12\n      40\n      52\n    \n    \n      2\n      Tea\n      88\n      60\n      63\n      98\n      48\n      86\n      99\n      77\n      61\n      85\n      93\n      92\n      83\n      84\n      40\n      99\n    \n    \n      3\n      Sweetener\n      19\n      2\n      4\n      32\n      11\n      28\n      22\n      2\n      15\n      25\n      31\n      35\n      13\n      20\n      18\n      11\n    \n    \n      4\n      Biscuits\n      57\n      55\n      76\n      62\n      74\n      79\n      91\n      22\n      29\n      31\n      61\n      66\n      62\n      64\n      62\n      80\n    \n    \n      5\n      Powder soup\n      51\n      41\n      53\n      67\n      37\n      73\n      55\n      34\n      33\n      69\n      43\n      32\n      51\n      27\n      43\n      75\n    \n    \n      6\n      Tin soup\n      19\n      3\n      11\n      43\n      23\n      12\n      76\n      1\n      1\n      10\n      43\n      17\n      4\n      10\n      2\n      18\n    \n    \n      7\n      Potatoes\n      21\n      2\n      23\n      7\n      9\n      7\n      17\n      5\n      5\n      17\n      39\n      11\n      17\n      8\n      14\n      2\n    \n    \n      8\n      Frozen fish\n      27\n      4\n      11\n      14\n      13\n      26\n      20\n      20\n      15\n      19\n      54\n      51\n      30\n      18\n      23\n      5\n    \n    \n      9\n      Frozen veggies\n      21\n      2\n      5\n      14\n      12\n      23\n      24\n      3\n      11\n      15\n      45\n      42\n      15\n      12\n      7\n      3\n    \n    \n      10\n      Apples\n      81\n      67\n      87\n      83\n      76\n      85\n      76\n      22\n      49\n      79\n      56\n      81\n      61\n      50\n      59\n      57\n    \n    \n      11\n      Oranges\n      75\n      71\n      84\n      89\n      76\n      94\n      68\n      51\n      42\n      70\n      78\n      72\n      72\n      57\n      77\n      52\n    \n    \n      12\n      Tinned fruit\n      44\n      9\n      40\n      61\n      42\n      83\n      89\n      8\n      14\n      46\n      53\n      50\n      34\n      22\n      30\n      46\n    \n    \n      13\n      Jam\n      71\n      46\n      45\n      81\n      57\n      20\n      91\n      16\n      41\n      61\n      75\n      64\n      51\n      37\n      38\n      89\n    \n    \n      14\n      Garlic\n      22\n      80\n      88\n      15\n      29\n      91\n      11\n      89\n      51\n      64\n      9\n      11\n      11\n      15\n      86\n      5\n    \n    \n      15\n      Butter\n      91\n      66\n      94\n      31\n      84\n      94\n      95\n      65\n      51\n      82\n      68\n      92\n      63\n      96\n      44\n      97\n    \n    \n      16\n      Margarine\n      85\n      24\n      47\n      97\n      80\n      94\n      94\n      78\n      72\n      48\n      32\n      91\n      94\n      94\n      51\n      25\n    \n    \n      17\n      Olive oil\n      74\n      94\n      36\n      13\n      83\n      84\n      57\n      92\n      28\n      61\n      48\n      30\n      28\n      17\n      91\n      31\n    \n    \n      18\n      Yoghurt\n      30\n      5\n      57\n      53\n      20\n      31\n      11\n      6\n      13\n      48\n      2\n      11\n      2\n      21\n      16\n      3\n    \n    \n      19\n      Crisp bread\n      26\n      18\n      3\n      15\n      5\n      24\n      28\n      9\n      11\n      30\n      93\n      34\n      62\n      64\n      13\n      9"
  },
  {
    "objectID": "posts/PCA_eu_food.html#normalize-data-1",
    "href": "posts/PCA_eu_food.html#normalize-data-1",
    "title": "Ryan Keeney",
    "section": "Normalize data",
    "text": "Normalize data\n\n# In this case, we normalize the data because features have very different ranges\nstdA = np.std(Anew,axis = 0)\nAnew = Anew @ np.diag(np.ones(stdA.shape[0])/stdA)\nAnew = Anew.T\n\n#display(pd.DataFrame(Anew))"
  },
  {
    "objectID": "posts/PCA_eu_food.html#pca-1",
    "href": "posts/PCA_eu_food.html#pca-1",
    "title": "Ryan Keeney",
    "section": "PCA",
    "text": "PCA\n\nmu = np.mean(Anew,axis = 1)\nxc = Anew - mu[:,None]\n\nC = np.dot(xc,xc.T)/m\n\nK=2\nS,W = ll.eigs(C,k = K)\nS = S.real\nW = W.real\n\ndim1 = np.dot(W[:,0].T,xc)/math.sqrt(S[0]) # extract 1st eigenvalues\ndim2 = np.dot(W[:,1].T,xc)/math.sqrt(S[1]) # extract 2nd eigenvalue\n\n(20,)"
  },
  {
    "objectID": "posts/PCA_eu_food.html#plot-2-common-and-uncommon-pairings",
    "href": "posts/PCA_eu_food.html#plot-2-common-and-uncommon-pairings",
    "title": "Ryan Keeney",
    "section": "Plot 2: Common (and uncommon) pairings",
    "text": "Plot 2: Common (and uncommon) pairings\nThe location of the foods does seem logical (similar foods or ‚Äòcompatible‚Äô foods are grouped together). Fresh fruit such as apples and oranges are grouped. So are common toppings such as jam, margarine, butter, and what they are put on (biscuits). Garlic and olive oil are loosely grouped and separated. Visually inspecting the graph, it seems that PC1 can be described as fish vs.¬†high caffine (coffee, tea) intake and PC2 can be described by how much garlic and olive oil are apart of the diet for a country.\n\nplt.figure(figsize=(7, 7))\nplt.xlabel(\"dim1\")\nplt.ylabel(\"dim2\")\nplt.plot(dim1, dim2, 'r.')\nplt.axvline(0, linewidth=0.5)\nplt.axhline(0, linewidth=0.5)\n\n# add labels\nfor i in range(0, food_items.shape[0]):\n    plt.text(dim1[i], dim2[i] + 0.01, food_items[i])\n\nplt.show()"
  },
  {
    "objectID": "posts/PCA_eu_food.html#check-correlations-to-dim1-and-dim2-1",
    "href": "posts/PCA_eu_food.html#check-correlations-to-dim1-and-dim2-1",
    "title": "Ryan Keeney",
    "section": "Check correlations to dim1 and dim2",
    "text": "Check correlations to dim1 and dim2\nDimension 1 (PC 1) is highly correlated to food consumption areas like Germany, although it also has mild to strong correlations to many of the countries ‚Äì indicating that this is indeed the dimension upon variation is maximized. Dimension 2 is most highly correlated to Sweden (positive) and Spain (negative).\n\n# add dim1 and dim2\nfood_consumption['dim1'] = dim1.tolist()\nfood_consumption['dim2'] = dim2.tolist()\ndisplay(food_consumption.drop(columns=['Food Item']).corr(method='pearson').tail(2).style.format('{0:,.2f}'))\n\n\n                    Germany        Italy        France        Holland        Belgium        Luxembourg        England        Portugal        Austria        Switzerland        Sweden        Denmark        Norway        Finland        Spain        Ireland        dim1        dim2    \n                \n                        dim1\n                        0.95\n                        0.80\n                        0.80\n                        0.70\n                        0.90\n                        0.86\n                        0.60\n                        0.73\n                        0.89\n                        0.82\n                        0.49\n                        0.82\n                        0.84\n                        0.80\n                        0.73\n                        0.70\n                        1.00\n                        0.00\n            \n            \n                        dim2\n                        0.13\n                        -0.42\n                        -0.32\n                        0.33\n                        -0.07\n                        -0.33\n                        0.40\n                        -0.49\n                        -0.13\n                        -0.24\n                        0.56\n                        0.43\n                        0.38\n                        0.35\n                        -0.60\n                        0.31\n                        0.00\n                        1.00"
  },
  {
    "objectID": "posts/tidymodels_recipes.html",
    "href": "posts/tidymodels_recipes.html",
    "title": "Preprocessing data in R",
    "section": "",
    "text": "Cleaning and preprocessing data is an essential skill for data scientists. Luckily, tidymodels makes it easy and has become my go-to process for building robust and sustainable workflows.\nMy favorite advantage using this approach is that I can directly tie my preprocessing steps into my model. No more building and rebuilding preprocessing and modeling steps if you decide to change something on either end.\nLearn more: https://www.tidymodels.org/start/recipes/"
  },
  {
    "objectID": "posts/tidymodels_recipes.html#functions",
    "href": "posts/tidymodels_recipes.html#functions",
    "title": "Preprocessing data in R",
    "section": "Functions",
    "text": "Functions\nFor a full list of functions, please visit https://recipes.tidymodels.org/reference/index.html\n\nSelecting variables within functions\nThe recipes package has a helpful set of methods for selecting variables within step functions. It uses dplyr like syntax, and helpers from the tidyselect package such as starts_with() or contains(). Additionally, you can select roles, or classes of variables.\n\n\nRoles\nWhile some roles set up during the formula process, roles can be manually altered as well. I‚Äôll often use these functions to set up ID variables, which can be kept apart of the data, but ignored during modeling tasks. No more splitting and re-joining! Once a role is selected, you can select them in future function steps such as all_outcomes(), all_predictors(), or be even more specific with selections like all_integer_predictors().\n\n# update variable 'vehicle' to be an id variable.\nbasic_recipe <- basic_recipe |>  update_role('vehicle',new_role = 'id variable')\nsummary(basic_recipe) |> knitr::kable()\n\n\n\n\nvariable\ntype\nrole\nsource\n\n\n\n\nvehicle\nstring , unordered, nominal\nid variable\noriginal\n\n\ncyl\ndouble , numeric\npredictor\noriginal\n\n\ndisp\ndouble , numeric\npredictor\noriginal\n\n\nhp\ndouble , numeric\npredictor\noriginal\n\n\ndrat\ndouble , numeric\npredictor\noriginal\n\n\nwt\ndouble , numeric\npredictor\noriginal\n\n\nqsec\ndouble , numeric\npredictor\noriginal\n\n\nvs\ndouble , numeric\npredictor\noriginal\n\n\nam\ndouble , numeric\npredictor\noriginal\n\n\ngear\ndouble , numeric\npredictor\noriginal\n\n\ncarb\ndouble , numeric\npredictor\noriginal\n\n\nmpg\ndouble , numeric\noutcome\noriginal\n\n\n\n\n\n\n\nImputation\nAnother powerful feature of the recipes package is the imputation tools. If our data has NAs (it doesn‚Äôt) we could impute them using methods such as mean, mode, bagged trees, KNN, linear model, or even assign categories to ‚Äúunknown‚Äù.\nLet‚Äôs induce some NAs and then see how well it works.\n\n# randomly set 5 vehicle's hp to NA\ndata_missing <- data\ndata_missing$hp[sample(1:nrow(data), 5)] <- NA\n\n# check\ndata_missing |> filter(is.na(hp)) |> knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvehicle\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nHornet Sportabout\n18.7\n8\n360.0\nNA\n3.15\n3.44\n17.02\n0\n0\n3\n2\n\n\nMerc 280\n19.2\n6\n167.6\nNA\n3.92\n3.44\n18.30\n1\n0\n4\n4\n\n\nCadillac Fleetwood\n10.4\n8\n472.0\nNA\n2.93\n5.25\n17.98\n0\n0\n3\n4\n\n\nFiat 128\n32.4\n4\n78.7\nNA\n4.08\n2.20\n19.47\n1\n1\n4\n1\n\n\nFerrari Dino\n19.7\n6\n145.0\nNA\n3.62\n2.77\n15.50\n0\n1\n5\n6\n\n\n\n\n\n\n# create, imputation recipe and add imputation step function  \nimputed_recipe <- recipe(mpg ~., data = train_data) |> \n  update_role('vehicle',new_role = 'id variable') |> \n  step_impute_linear(hp, impute_with = imp_vars(cyl,disp))\n\n# fit recipe\nimputed_fit <- imputed_recipe |> prep(data_missing)\nimputed_fit |> print()\n\nRecipe\n\nInputs:\n\n        role #variables\n id variable          1\n     outcome          1\n   predictor         10\n\nTraining data contained 32 data points and 5 incomplete rows. \n\nOperations:\n\nLinear regression imputation for hp [trained]\n\n# apply recipe to the missing data\nimputed_data <- bake(object = imputed_fit, new_data = data_missing) |> \n  rename(hp_imputed = hp) |> \n  bind_cols(data |> select(hp_original = hp)) |> \n  bind_cols(data_missing |> select(hp)) |> \n  filter(is.na(hp))\n\n# check\nimputed_data |> select(vehicle,hp_imputed,hp_original) |> knitr::kable()\n\n\n\n\nvehicle\nhp_imputed\nhp_original\n\n\n\n\nHornet Sportabout\n210.72734\n175\n\n\nMerc 280\n131.11972\n123\n\n\nCadillac Fleetwood\n233.18042\n205\n\n\nFiat 128\n72.26115\n66\n\n\nFerrari Dino\n126.58901\n175\n\n\n\n\n\n\n# plot check\nggplot(imputed_data, aes(x=hp_original, y = hp_imputed)) +\n  geom_abline() +\n  geom_point() + \n  labs(title='Imputed Values', x='Original',y='Imputed') +\n  theme_bw()\n\n\n\n\n\n\nIndividual Transformations\nIndividual transformations can be performed, and are especially helpful for certain models that require certain assumptions to hold. For example, sometimes it‚Äôd be nice to transform a non-normal distribution into a normal distribution, enter the step_YeoJohnson() function. Other times, there are non-linear relationships that should be adjusted, especially if the model selected expects them to be.\n\nlibrary(cowplot)\n\n\np1 <- ggplot(data, aes(x=hp, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = 'lm', formula =y ~ x,color='red',se = FALSE) +\n  geom_smooth(method = 'loess', formula =y ~ x) +\n  theme_bw() +\n  labs(title = 'No hp transformation')\n\n\n\np2 <- ggplot(data, aes(x=log(hp), y = mpg)) +\n  geom_point() +\n  geom_smooth(method = 'lm', formula =y ~ x,color='red',se = FALSE) +\n  geom_smooth(method = 'loess', formula =y ~ x) +\n  theme_bw() +\n  labs(title = 'Log(hp) transformation',y='')\n\nplot_grid(p1, p2)\n\n\n\n\n\n# add log step to basic recipe\nbasic_recipe <- basic_recipe |> \n  step_log(hp)\n\nTransformations of variables should be carefully considered, intentionally selected, and properly evaluated. Some methods (linear regression, PCA) require closer attention to the inputs than, say XGBoost. Quick note, normalization is covered below.\n\n\nDiscretization\nYou can transform continuous variables into discrete variables with these step functions. That said, you really need to have a good reason for doing this to predicting variables. Here are just a few problems caused by categorizing continuous variables. If you insist on continuing, check out step_discretize() and step_cut().\n\n\nDummy Variables and Encoding\nProbably the most commonly used functions from this set are step_dummy() and step_date() or step_holiday() for working with time series data. Creating dummy variables explicitly before passing the data into a model gives you additional control and also sets names that are easier to interpret, set one_hot=TRUE to make sure every category gets encoded.\nExample of step_dummy()\n\n# There are three cylinder sizes in the dataset\ndata$cyl |> unique() |> sort()\n\n[1] 4 6 8\n\n\nWithout one-hot encoding\n\ndummies_recipe <- recipe(mpg ~., data = train_data) |> \n  update_role('vehicle',new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |> #individual transformation: numeric -> factor\n  step_dummy(cyl) |> # new dummy step\n  prep(train_data) # fit\n\n# apply the recipe to the data\ndummies_data <- bake(dummies_recipe, new_data=NULL) \n\ndummies_data |> select(starts_with('cyl')) |> names()\n\n[1] \"cyl_X6\" \"cyl_X8\"\n\n\nWith one-hot encoding\n\ndummies_recipe <- recipe(mpg ~., data = train_data) |> \n  update_role('vehicle',new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |> #individual transformation: numeric -> factor\n  step_dummy(cyl, one_hot = TRUE) |> # new dummy step with one hot\n  prep(train_data) # fit\n\n# apply the recipe to the data\ndummies_data <- bake(dummies_recipe, new_data=NULL) \n\ndummies_data |> select(starts_with('cyl')) |> names() \n\n[1] \"cyl_X4\" \"cyl_X6\" \"cyl_X8\"\n\n\n\n\nInteractions\nstep_interact() creates an interaction between variables. It is primarily intended for numeric data, or categorical data that has been converted to a dummy step.\n\ninteract_recipe <- recipe(mpg ~., data = data) |> \n  update_role('vehicle',new_role = 'id variable') |> \n  # create interaction between weight and horsepower\n  step_interact(terms = ~ hp:wt) |> \n  prep(data)\n\ninteract_data <- bake(interact_recipe,new_data = NULL)\n\ninteract_data |> select(hp,wt,hp_x_wt) |> head(3) |> knitr::kable()\n\n\n\n\nhp\nwt\nhp_x_wt\n\n\n\n\n110\n2.620\n288.20\n\n\n110\n2.875\n316.25\n\n\n93\n2.320\n215.76\n\n\n\n\n\n\n\nNormalization\nNormalization step functions are probably the most commonly used and are often necessary for accurate modeling when using methods like PCA or regularized regression such as LASSO.\n\nstep_center() Centers numeric data\nstep_normalize() Centers and scales numeric data (this is probably the one you want!)\nstep_range() Scale numeric data to a specific range, helpful for converting scales 1-5 or 0-100, although sometimes step_percentile() is a better fit.\nstep_scale() Scale numeric data\n\nSince it‚Äôs common to apply normalization to all numeric variables, you can select them quickly using all_numeric() or all_numeric_predictors().\n\nnormalize_recipe <- recipe(mpg ~., data = data) |> \n  update_role('vehicle',new_role = 'id variable') |> \n  step_mutate(cyl = as.factor(cyl)) |>\n  # select the vars you want, or just grap all the numeric ones.\n  step_normalize(all_numeric_predictors()) |> \n  prep(data)\n\nnormalize_data <- bake(normalize_recipe, new_data = NULL)\n\n# notice that it skips the cyl (factor) and mpg (outcome) columns!\nnormalize_data |> select(vehicle,cyl, disp,hp,mpg) |> head(3) |> knitr::kable()\n\n\n\n\nvehicle\ncyl\ndisp\nhp\nmpg\n\n\n\n\nMazda RX4\n6\n-0.5706198\n-0.5350928\n21.0\n\n\nMazda RX4 Wag\n6\n-0.5706198\n-0.5350928\n21.0\n\n\nDatsun 710\n4\n-0.9901821\n-0.7830405\n22.8\n\n\n\n\n\n\n\nMultivariate Transformations\nMany different multivariate transformations are available, from geospatial distance functions to kernel PCA functions. Even one of my favorite algorithms, step_isomap(), is available!\nHere‚Äôs and example using step_pca(). This format is so easy, I often prefer it to more specialized packages when performing EDA or dimensional reduction.\n\npca_recipe <- recipe(mpg ~hp+wt+cyl+drat+qsec+vehicle, data = data) |>\n  # include vehicle in formula, but then set as ID to keep it.\n  update_role('vehicle', new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |>\n  step_normalize(all_numeric_predictors()) |>  # necessary for PCA!\n  step_pca(all_numeric_predictors(),num_comp = 2) |>  # PCA, keep the top 2 components\n  prep(data)\n\npca_data <- bake(pca_recipe, new_data = NULL)\n\n# cylinders is not included in the PCA because it is a factor\n# mpg is not included because it is an outcome.\npca_data |> head(3) |> knitr::kable()\n\n\n\n\ncyl\nvehicle\nmpg\nPC1\nPC2\n\n\n\n\n6\nMazda RX4\n21.0\n-0.6150518\n-0.9200350\n\n\n6\nMazda RX4 Wag\n21.0\n-0.5925301\n-0.5983580\n\n\n4\nDatsun 710\n22.8\n-1.3388038\n-0.0468172\n\n\n\n\n\n\n\nFilters\nFilter step functions are a great way to ‚Äòautomate‚Äô your modeling workflow - that is, to place all your preprocessing steps within your recipe. Want to remove pesky columns that have near-zero variance? step_nzv() does that. Want to control for highly correlated columns? step_corr() is here to help. There are many different filters to choose from here, all are useful for ensuring your workflow can handle different scenarios.\nExample of step_filter_missing()\n\n# create missing data\ndata_missing <- data\n\n# randomly set 5 vehicle's hp to NA\ndata_missing$hp[sample(1:nrow(data), 5)] <- NA\n\n# randomly set 10 vehicle's wt to NA\ndata_missing$wt[sample(1:nrow(data), 10)] <- NA\n\n#check\ndata_missing |> filter(is.na(hp) | is.na(wt)) |> head(3) |> knitr::kable()\n\n\n\n\nvehicle\nmpg\ncyl\ndisp\nhp\ndrat\nwt\nqsec\nvs\nam\ngear\ncarb\n\n\n\n\nValiant\n18.1\n6\n225.0\n105\n2.76\nNA\n20.22\n1\n0\n3\n1\n\n\nMerc 240D\n24.4\n4\n146.7\n62\n3.69\nNA\n20.00\n1\n0\n4\n2\n\n\nMerc 230\n22.8\n4\n140.8\n95\n3.92\nNA\n22.90\n1\n0\n4\n2\n\n\n\n\n\n\nfilter_recipe <- recipe(mpg ~hp+wt+cyl+vehicle, data = data_missing) |>\n  # include vehicle in formula, but then set as ID to keep it.\n  update_role('vehicle', new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |>\n  # remove columns with more than 20% missing values\n  step_filter_missing(all_predictors(),threshold=.2) |> \n  prep(data_missing)\n\n# wt is removed (exceeds threshold), while hp is not\nfilter_recipe |> print()\n\nRecipe\n\nInputs:\n\n        role #variables\n id variable          1\n     outcome          1\n   predictor          3\n\nTraining data contained 32 data points and 11 incomplete rows. \n\nOperations:\n\nVariable mutation for ~as.factor(cyl) [trained]\nMissing value column filter removed wt [trained]\n\n\n\n\nRow Operations\nRow operations are mostly extensions from dplyr with functions such as step_filter() and step_naomit(). Again, the goal is to build the typical preparation and cleaning operations into your workflow. Expect a common input format - then make it useful/understandable.\n\n\nOther Step Functions\nThere are a few miscellaneous step functions that doesn‚Äôt fall within the normal in the organization structure set up in. A particularly useful one is step_rename() for dplyr-like renaming and step_window() for window functions.\n\n\nCheck Functions\nCheck functions are useful for identifying issues before progressing to more intensive steps. If the check fails, it will break the bake function and give an error.\nExample of check_missing()\n\n# reuse missing data.. as expected, gives error.\ncheck_recipe <- recipe(mpg ~hp+wt+cyl+vehicle, data = data_missing) |>\n  # include vehicle in formula, but then set as ID to keep it.\n  update_role('vehicle', new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |>\n  # create error if there are any missing values in the predicting values\n  check_missing(all_predictors()) |> \n  prep(data_missing)\n\nError in `bake()`:\n! The following columns contain missing values: `hp`, `wt`.\n\n\n\n\nInternal Step Handling\nFinally, there are a few functions that help manage the naming of variables, adding steps or checks, and inspecting recipes. Two ones that I use to debug are detect_step() and fully_trained().\n\ninternal_recipe <-recipe(mpg ~hp+wt+cyl+vehicle, data = data) |>\n  update_role('vehicle', new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |>\n  step_BoxCox(all_numeric())\n\n# is the recipe trained/fit? false\ninternal_recipe |> fully_trained()\n\n[1] FALSE\n\n\n\n# ok, train it\ninternal_recipe <- internal_recipe |> prep(data)\n\n# is the recipe fit? true!\ninternal_recipe |> fully_trained()\n\n[1] TRUE\n\n\n\n# did the recipe use step_Yeo_Johnson? false\ninternal_recipe |> detect_step('YeoJohnson')\n\n[1] FALSE\n\n\n\n# did it use step_BoxCox? true\ninternal_recipe |> detect_step('BoxCox')\n\n[1] TRUE"
  }
]