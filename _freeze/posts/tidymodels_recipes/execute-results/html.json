{
  "hash": "12760d5ec85c39bb487abb6b9bbab213",
  "result": {
    "markdown": "---\ntitle: \"Preprocessing data in R\"\nauthor: \"Ryan Keeney\"\neditor: visual\nexecute:\n  freeze: auto\n---\n\n\n# Introduction\n\nCleaning and preprocessing data is an essential skill for data scientists. Luckily, [tidymodels](https://www.tidymodels.org/) makes it easy and has become my go-to process for building robust and sustainable workflows.\n\nMy favorite advantage using this approach is that I can directly tie my preprocessing steps into my model. No more building and rebuilding preprocessing and modeling steps if you decide to change something on either end.\n\nLearn more: <https://www.tidymodels.org/start/recipes/>\n\n# Libraries and Example Data\n\nLoad the tidymodels package and the mtcars data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# libraries\nlibrary(tidymodels, quietly = TRUE, warn.conflicts = FALSE)\n\n# load data, covert row name to column\ndata <- mtcars |> rownames_to_column(var = 'vehicle')\nknitr::kable(data |> sample_n(3))\n```\n\n::: {.cell-output-display}\n|vehicle       |  mpg| cyl|  disp|  hp| drat|   wt| qsec| vs| am| gear| carb|\n|:-------------|----:|---:|-----:|---:|----:|----:|----:|--:|--:|----:|----:|\n|Merc 240D     | 24.4|   4| 146.7|  62| 3.69| 3.19| 20.0|  1|  0|    4|    2|\n|Maserati Bora | 15.0|   8| 301.0| 335| 3.54| 3.57| 14.6|  0|  1|    5|    8|\n|Ferrari Dino  | 19.7|   6| 145.0| 175| 3.62| 2.77| 15.5|  0|  1|    5|    6|\n:::\n:::\n\n\n# Splitting Data\n\nIf you take one thing away from this post, it'll likely be the these three functions. Splitting data (with built in methods for controlling breaks and strata) is simple with tidymodels.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for replication\nset.seed(1212022)\n\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(data, prop = 3/4, strata = cyl)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n```\n:::\n\n\nNote: rsample can be used to set up cross validation, bootstrap resampling, and more. See <https://www.tidymodels.org/tags/rsample/> for examples.\n\n# Recipes\n\n**Basic Functions:**\n\n-   [recipe()](https://recipes.tidymodels.org/reference/recipe.html) Create a recipe for preprocessing data\n\n-   [formula(*\\<recipe\\>*)](https://recipes.tidymodels.org/reference/formula.recipe.html) Create a Formula from a Prepared Recipe\n\n-   [print(*\\<recipe\\>*)](https://recipes.tidymodels.org/reference/print.recipe.html) Print a Recipe\n\n-   [summary(*\\<recipe\\>*)](https://recipes.tidymodels.org/reference/summary.recipe.html) Summarize a recipe\n\n-   [prep()](https://recipes.tidymodels.org/reference/prep.html) Estimate (e.g., \"fit,\"train\") a preprocessing recipe\n\n-   [bake()](https://recipes.tidymodels.org/reference/bake.html) Apply a trained preprocessing recipe\n\nSome preprocessing steps require you to \"fit\" or \"trained\" on data before it can applied to new data. If you are familiar with sklearn.preprocessing functions in **Python**, then these concepts might confuse you in name only - the same process still applies. If your goal is to eventually use the recipe as a preprocessor in modeling, it is suggested that a workflow is used instead of manually estimating a recipe with prep().\n\nLet's build a basic recipe, that references a formula \\<mpg \\~.\\>(predict mpg using all other features) and uses the training data.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbasic_recipe <- recipe(mpg ~., data = train_data)\n```\n:::\n\n\nRecipes can be piped so that multiple step functions can be included. In fact, that's a requirement for us here because I included the vehicle name as a feature. Let's see what sort of functions we can apply.\n\n## Functions\n\nFor a full list of functions, please visit <https://recipes.tidymodels.org/reference/index.html>\n\n### Selecting variables within functions\n\nThe recipes package has a helpful set of methods for selecting variables within step functions. It uses dplyr like syntax, and helpers from the [tidyselect](https://tidyselect.r-lib.org/index.html) package such as [starts_with()](https://tidyselect.r-lib.org/reference/starts_with.html) or [contains()](https://tidyselect.r-lib.org/reference/starts_with.html). Additionally, you can select roles, or classes of variables.\n\n### Roles\n\nWhile some roles set up during the formula process, roles can be manually altered as well. I'll often use these functions to set up ID variables, which can be kept apart of the data, but ignored during modeling tasks. No more splitting and re-joining! Once a role is selected, you can select them in future function steps such as [all_outcomes()](https://recipes.tidymodels.org/reference/has_role.html), [all_predictors()](https://recipes.tidymodels.org/reference/has_role.html), or be even more specific with selections like [all_integer_predictors()](https://recipes.tidymodels.org/reference/has_role.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# update variable 'vehicle' to be an id variable.\nbasic_recipe <- basic_recipe |>  update_role('vehicle',new_role = 'id variable')\nsummary(basic_recipe) |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|variable |type                          |role        |source   |\n|:--------|:-----------------------------|:-----------|:--------|\n|vehicle  |string   , unordered, nominal |id variable |original |\n|cyl      |double , numeric              |predictor   |original |\n|disp     |double , numeric              |predictor   |original |\n|hp       |double , numeric              |predictor   |original |\n|drat     |double , numeric              |predictor   |original |\n|wt       |double , numeric              |predictor   |original |\n|qsec     |double , numeric              |predictor   |original |\n|vs       |double , numeric              |predictor   |original |\n|am       |double , numeric              |predictor   |original |\n|gear     |double , numeric              |predictor   |original |\n|carb     |double , numeric              |predictor   |original |\n|mpg      |double , numeric              |outcome     |original |\n:::\n:::\n\n\n### Imputation\n\nAnother powerful feature of the recipes package is the imputation tools. If our data has NAs (it doesn't) we could impute them using methods such as mean, mode, bagged trees, KNN, linear model, or even assign categories to \"unknown\".\n\nLet's induce some NAs and then see how well it works.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# randomly set 5 vehicle's hp to NA\ndata_missing <- data\ndata_missing$hp[sample(1:nrow(data), 5)] <- NA\n\n# check\ndata_missing |> filter(is.na(hp)) |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|vehicle            |  mpg| cyl|  disp| hp| drat|   wt|  qsec| vs| am| gear| carb|\n|:------------------|----:|---:|-----:|--:|----:|----:|-----:|--:|--:|----:|----:|\n|Hornet Sportabout  | 18.7|   8| 360.0| NA| 3.15| 3.44| 17.02|  0|  0|    3|    2|\n|Merc 280           | 19.2|   6| 167.6| NA| 3.92| 3.44| 18.30|  1|  0|    4|    4|\n|Cadillac Fleetwood | 10.4|   8| 472.0| NA| 2.93| 5.25| 17.98|  0|  0|    3|    4|\n|Fiat 128           | 32.4|   4|  78.7| NA| 4.08| 2.20| 19.47|  1|  1|    4|    1|\n|Ferrari Dino       | 19.7|   6| 145.0| NA| 3.62| 2.77| 15.50|  0|  1|    5|    6|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# create, imputation recipe and add imputation step function  \nimputed_recipe <- recipe(mpg ~., data = train_data) |> \n  update_role('vehicle',new_role = 'id variable') |> \n  step_impute_linear(hp, impute_with = imp_vars(cyl,disp))\n\n# fit recipe\nimputed_fit <- imputed_recipe |> prep(data_missing)\nimputed_fit |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n        role #variables\n id variable          1\n     outcome          1\n   predictor         10\n\nTraining data contained 32 data points and 5 incomplete rows. \n\nOperations:\n\nLinear regression imputation for hp [trained]\n```\n:::\n\n```{.r .cell-code}\n# apply recipe to the missing data\nimputed_data <- bake(object = imputed_fit, new_data = data_missing) |> \n  rename(hp_imputed = hp) |> \n  bind_cols(data |> select(hp_original = hp)) |> \n  bind_cols(data_missing |> select(hp)) |> \n  filter(is.na(hp))\n\n# check\nimputed_data |> select(vehicle,hp_imputed,hp_original) |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|vehicle            | hp_imputed| hp_original|\n|:------------------|----------:|-----------:|\n|Hornet Sportabout  |  210.72734|         175|\n|Merc 280           |  131.11972|         123|\n|Cadillac Fleetwood |  233.18042|         205|\n|Fiat 128           |   72.26115|          66|\n|Ferrari Dino       |  126.58901|         175|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# plot check\nggplot(imputed_data, aes(x=hp_original, y = hp_imputed)) +\n  geom_abline() +\n  geom_point() + \n  labs(title='Imputed Values', x='Original',y='Imputed') +\n  theme_bw()\n```\n\n::: {.cell-output-display}\n![](tidymodels_recipes_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n:::\n\n\n### Individual Transformations\n\nIndividual transformations can be performed, and are especially helpful for certain models that require certain assumptions to hold. For example, sometimes it'd be nice to transform a non-normal distribution into a normal distribution, enter the [step_YeoJohnson()](https://recipes.tidymodels.org/reference/step_YeoJohnson.html) function. Other times, there are non-linear relationships that should be adjusted, especially if the model selected expects them to be.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(cowplot)\n\n\np1 <- ggplot(data, aes(x=hp, y = mpg)) +\n  geom_point() +\n  geom_smooth(method = 'lm', formula =y ~ x,color='red',se = FALSE) +\n  geom_smooth(method = 'loess', formula =y ~ x) +\n  theme_bw() +\n  labs(title = 'No hp transformation')\n\n\n\np2 <- ggplot(data, aes(x=log(hp), y = mpg)) +\n  geom_point() +\n  geom_smooth(method = 'lm', formula =y ~ x,color='red',se = FALSE) +\n  geom_smooth(method = 'loess', formula =y ~ x) +\n  theme_bw() +\n  labs(title = 'Log(hp) transformation',y='')\n\nplot_grid(p1, p2)\n```\n\n::: {.cell-output-display}\n![](tidymodels_recipes_files/figure-html/unnamed-chunk-16-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# add log step to basic recipe\nbasic_recipe <- basic_recipe |> \n  step_log(hp)\n```\n:::\n\n\nTransformations of variables should be carefully considered, intentionally selected, and properly evaluated. Some methods (linear regression, PCA) require closer attention to the inputs than, say XGBoost. Quick note, [normalization] is covered below.\n\n### Discretization\n\nYou can transform continuous variables into discrete variables with these step functions. That said, you *really* need to have a good reason for doing this to predicting variables. Here are just a [few problems](https://discourse.datamethods.org/t/categorizing-continuous-variables/3402) caused by categorizing continuous variables. If you insist on continuing, check out [step_discretize()](https://recipes.tidymodels.org/reference/step_discretize.html) and [step_cut()](https://recipes.tidymodels.org/reference/step_cut.html).\n\n### Dummy Variables and Encoding\n\nProbably the most commonly used functions from this set are [step_dummy()](https://recipes.tidymodels.org/reference/step_dummy.html) and [step_date()](https://recipes.tidymodels.org/reference/step_date.html) or [step_holiday()](https://recipes.tidymodels.org/reference/step_holiday.html) for working with time series data. Creating dummy variables explicitly before passing the data into a model gives you additional control and also sets names that are easier to interpret, set one_hot=TRUE to make sure every category gets encoded.\n\nExample of [step_dummy()](https://recipes.tidymodels.org/reference/step_dummy.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# There are three cylinder sizes in the dataset\ndata$cyl |> unique() |> sort()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 4 6 8\n```\n:::\n:::\n\n\nWithout one-hot encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndummies_recipe <- recipe(mpg ~., data = train_data) |> \n  update_role('vehicle',new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |> #individual transformation: numeric -> factor\n  step_dummy(cyl) |> # new dummy step\n  prep(train_data) # fit\n\n# apply the recipe to the data\ndummies_data <- bake(dummies_recipe, new_data=NULL) \n\ndummies_data |> select(starts_with('cyl')) |> names()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"cyl_X6\" \"cyl_X8\"\n```\n:::\n:::\n\n\nWith one-hot encoding\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndummies_recipe <- recipe(mpg ~., data = train_data) |> \n  update_role('vehicle',new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |> #individual transformation: numeric -> factor\n  step_dummy(cyl, one_hot = TRUE) |> # new dummy step with one hot\n  prep(train_data) # fit\n\n# apply the recipe to the data\ndummies_data <- bake(dummies_recipe, new_data=NULL) \n\ndummies_data |> select(starts_with('cyl')) |> names() \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"cyl_X4\" \"cyl_X6\" \"cyl_X8\"\n```\n:::\n:::\n\n\n### Interactions\n\n[step_interact()](https://recipes.tidymodels.org/reference/step_interact.html) creates an [interaction](https://www.theanalysisfactor.com/interpreting-interactions-in-regression/) between variables. It is primarily intended for numeric data, or categorical data that has been converted to a dummy step.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninteract_recipe <- recipe(mpg ~., data = data) |> \n  update_role('vehicle',new_role = 'id variable') |> \n  # create interaction between weight and horsepower\n  step_interact(terms = ~ hp:wt) |> \n  prep(data)\n\ninteract_data <- bake(interact_recipe,new_data = NULL)\n\ninteract_data |> select(hp,wt,hp_x_wt) |> head(3) |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|  hp|    wt| hp_x_wt|\n|---:|-----:|-------:|\n| 110| 2.620|  288.20|\n| 110| 2.875|  316.25|\n|  93| 2.320|  215.76|\n:::\n:::\n\n\n### Normalization\n\nNormalization step functions are probably the most commonly used and are often necessary for accurate modeling when using methods like PCA or regularized regression such as LASSO.\n\n-   [step_center()](https://recipes.tidymodels.org/reference/step_center.html) Centers numeric data\n\n-   [step_normalize()](https://recipes.tidymodels.org/reference/step_normalize.html) Centers and scales numeric data (**this is probably the one you want!**)\n\n-   [step_range()](https://recipes.tidymodels.org/reference/step_range.html) Scale numeric data to a specific range, helpful for converting scales 1-5 or 0-100, although sometimes [step_percentile()](https://recipes.tidymodels.org/reference/step_percentile.html) is a better fit.\n\n-   [step_scale()](https://recipes.tidymodels.org/reference/step_scale.html) Scale numeric data\n\nSince it's common to apply normalization to all numeric variables, you can select them quickly using [all_numeric()](https://recipes.tidymodels.org/reference/has_role.html) or [all_numeric_predictors()](https://recipes.tidymodels.org/reference/has_role.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormalize_recipe <- recipe(mpg ~., data = data) |> \n  update_role('vehicle',new_role = 'id variable') |> \n  step_mutate(cyl = as.factor(cyl)) |>\n  # select the vars you want, or just grap all the numeric ones.\n  step_normalize(all_numeric_predictors()) |> \n  prep(data)\n\nnormalize_data <- bake(normalize_recipe, new_data = NULL)\n\n# notice that it skips the cyl (factor) and mpg (outcome) columns!\nnormalize_data |> select(vehicle,cyl, disp,hp,mpg) |> head(3) |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|vehicle       |cyl |       disp|         hp|  mpg|\n|:-------------|:---|----------:|----------:|----:|\n|Mazda RX4     |6   | -0.5706198| -0.5350928| 21.0|\n|Mazda RX4 Wag |6   | -0.5706198| -0.5350928| 21.0|\n|Datsun 710    |4   | -0.9901821| -0.7830405| 22.8|\n:::\n:::\n\n\n### Multivariate Transformations\n\nMany different [multivariate transformations](https://recipes.tidymodels.org/reference/index.html#step-functions-multivariate-transformations) are available, from geospatial distance functions to kernel PCA functions. Even one of my favorite algorithms, [step_isomap()](https://recipes.tidymodels.org/reference/step_isomap.html), is available!\n\nHere's and example using [step_pca()](https://recipes.tidymodels.org/reference/step_pca.html). This format is so easy, I often prefer it to more specialized packages when performing EDA or dimensional reduction.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npca_recipe <- recipe(mpg ~hp+wt+cyl+drat+qsec+vehicle, data = data) |>\n  # include vehicle in formula, but then set as ID to keep it.\n  update_role('vehicle', new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |>\n  step_normalize(all_numeric_predictors()) |>  # necessary for PCA!\n  step_pca(all_numeric_predictors(),num_comp = 2) |>  # PCA, keep the top 2 components\n  prep(data)\n\npca_data <- bake(pca_recipe, new_data = NULL)\n\n# cylinders is not included in the PCA because it is a factor\n# mpg is not included because it is an outcome.\npca_data |> head(3) |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|cyl |vehicle       |  mpg|        PC1|        PC2|\n|:---|:-------------|----:|----------:|----------:|\n|6   |Mazda RX4     | 21.0| -0.6150518| -0.9200350|\n|6   |Mazda RX4 Wag | 21.0| -0.5925301| -0.5983580|\n|4   |Datsun 710    | 22.8| -1.3388038| -0.0468172|\n:::\n:::\n\n\n### Filters\n\nFilter step functions are a great way to 'automate' your modeling workflow - that is, to place all your preprocessing steps *within* your recipe. Want to remove pesky columns that have near-zero variance? [step_nzv()](https://recipes.tidymodels.org/reference/step_nzv.html) does that. Want to control for highly correlated columns? [step_corr()](https://recipes.tidymodels.org/reference/step_corr.html) is here to help. There are many different filters to choose from here, all are useful for ensuring your workflow can handle different scenarios.\n\nExample of [step_filter_missing()](https://recipes.tidymodels.org/reference/step_filter_missing.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# create missing data\ndata_missing <- data\n\n# randomly set 5 vehicle's hp to NA\ndata_missing$hp[sample(1:nrow(data), 5)] <- NA\n\n# randomly set 10 vehicle's wt to NA\ndata_missing$wt[sample(1:nrow(data), 10)] <- NA\n\n#check\ndata_missing |> filter(is.na(hp) | is.na(wt)) |> head(3) |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|vehicle   |  mpg| cyl|  disp|  hp| drat| wt|  qsec| vs| am| gear| carb|\n|:---------|----:|---:|-----:|---:|----:|--:|-----:|--:|--:|----:|----:|\n|Valiant   | 18.1|   6| 225.0| 105| 2.76| NA| 20.22|  1|  0|    3|    1|\n|Merc 240D | 24.4|   4| 146.7|  62| 3.69| NA| 20.00|  1|  0|    4|    2|\n|Merc 230  | 22.8|   4| 140.8|  95| 3.92| NA| 22.90|  1|  0|    4|    2|\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nfilter_recipe <- recipe(mpg ~hp+wt+cyl+vehicle, data = data_missing) |>\n  # include vehicle in formula, but then set as ID to keep it.\n  update_role('vehicle', new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |>\n  # remove columns with more than 20% missing values\n  step_filter_missing(all_predictors(),threshold=.2) |> \n  prep(data_missing)\n\n# wt is removed (exceeds threshold), while hp is not\nfilter_recipe |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n        role #variables\n id variable          1\n     outcome          1\n   predictor          3\n\nTraining data contained 32 data points and 11 incomplete rows. \n\nOperations:\n\nVariable mutation for ~as.factor(cyl) [trained]\nMissing value column filter removed wt [trained]\n```\n:::\n:::\n\n\n### Row Operations\n\nRow operations are mostly extensions from dplyr with functions such as [step_filter()](https://recipes.tidymodels.org/reference/step_filter.html) and [step_naomit()](https://recipes.tidymodels.org/reference/step_naomit.html). Again, the goal is to build the typical preparation and cleaning operations *into* your workflow. Expect a common input format - then make it useful/understandable.\n\n### Other Step Functions\n\nThere are a few miscellaneous step functions that doesn't fall within the normal in the organization structure set up in. A particularly useful one is [step_rename()](https://recipes.tidymodels.org/reference/step_rename.html) for dplyr-like renaming and [step_window()](https://recipes.tidymodels.org/reference/step_window.html) for window functions.\n\n### Check Functions\n\nCheck functions are useful for identifying issues before progressing to more intensive steps. If the check fails, it will break the `bake` function and give an error.\n\nExample of [check_missing()](https://recipes.tidymodels.org/reference/check_missing.html)\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# reuse missing data.. as expected, gives error.\ncheck_recipe <- recipe(mpg ~hp+wt+cyl+vehicle, data = data_missing) |>\n  # include vehicle in formula, but then set as ID to keep it.\n  update_role('vehicle', new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |>\n  # create error if there are any missing values in the predicting values\n  check_missing(all_predictors()) |> \n  prep(data_missing)\n```\n\n::: {.cell-output .cell-output-error}\n```\nError in `bake()`:\n! The following columns contain missing values: `hp`, `wt`.\n```\n:::\n:::\n\n\n### Internal Step Handling\n\nFinally, there are a few functions that help manage the naming of variables, adding steps or checks, and inspecting recipes. Two ones that I use to debug are [detect_step()](https://recipes.tidymodels.org/reference/detect_step.html) and [fully_trained()](https://recipes.tidymodels.org/reference/fully_trained.html).\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninternal_recipe <-recipe(mpg ~hp+wt+cyl+vehicle, data = data) |>\n  update_role('vehicle', new_role = 'id variable') |>  \n  step_mutate(cyl = as.factor(cyl)) |>\n  step_BoxCox(all_numeric())\n\n# is the recipe trained/fit? false\ninternal_recipe |> fully_trained()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# ok, train it\ninternal_recipe <- internal_recipe |> prep(data)\n\n# is the recipe fit? true!\ninternal_recipe |> fully_trained()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# did the recipe use step_Yeo_Johnson? false\ninternal_recipe |> detect_step('YeoJohnson')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] FALSE\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# did it use step_BoxCox? true\ninternal_recipe |> detect_step('BoxCox')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\n# Creating a Workflow\n\nLet's put a entire workflow together and build a small model. But before we do that using a workflow, I want to demonstrate the advantages of a workflow that integrates a the preprocessing steps vs. the typical approach of preprocessing the data first, then passing it to a model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for replication\nset.seed(1212022)\n\n# Put 3/4 of the data into the training set \ndata_split <- initial_split(data, prop = 3/4, strata = cyl)\n\n# Create data frames for the two sets:\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\n# set up final recipe\nfinal_recipe <- recipe(mpg~.,data = train_data) |> \n  # set some of the variables to \n  update_role(vehicle,am,vs,gear,qsec, new_role = 'id variable') |> \n  # set cyl as factor\n  step_mutate(cyl = as.factor(cyl)) |>\n  # dummy step cyl\n  step_dummy(cyl, one_hot = TRUE) |> \n  # take log of hp\n  step_log(hp)\n\n# check recipe\nfinal_recipe |> print()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRecipe\n\nInputs:\n\n        role #variables\n id variable          5\n     outcome          1\n   predictor          6\n\nOperations:\n\nVariable mutation for as.factor(cyl)\nDummy variables from cyl\nLog transformation on hp\n```\n:::\n:::\n\n\nFirst, let's show why using a workflow is preferred to manually using prep.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# manually train recipe\nfinal_recipe <- final_recipe |> prep(train_data)\n\n# check if trained: true\nfinal_recipe |> fully_trained()\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] TRUE\n```\n:::\n:::\n\n\nSet the model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set model engine\nlm_model <- linear_reg() |> set_engine('lm')\n```\n:::\n\n\nNormally, we'd preprocess the data and then pass it into the model training. However... this doesn't work very well when we're using more complicated preprocessing recipes. Notice that the model isn't using the formula and roles used in the data, it's just using the preprocessed data, not exactly what we intended.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# prep the data \ndata_prepped <- prep(final_recipe, train_data)\n\n# fit the model\nlinear_reg_fit <- fit(lm_model,mpg~.,data = juice(data_prepped))\n\n# wait. oh no.\nlinear_reg_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = mpg ~ ., data = data)\n\nCoefficients:\n               (Intercept)   vehicleCadillac Fleetwood  \n                  1.52e+01                   -4.80e+00  \n         vehicleCamaro Z28    vehicleChrysler Imperial  \n                 -1.90e+00                   -5.00e-01  \n         vehicleDatsun 710     vehicleDodge Challenger  \n                  7.60e+00                    3.00e-01  \n         vehicleDuster 360             vehicleFiat 128  \n                 -9.00e-01                    1.72e+01  \n          vehicleFiat X1-9       vehicleHornet 4 Drive  \n                  1.21e+01                    6.20e+00  \n  vehicleHornet Sportabout  vehicleLincoln Continental  \n                  3.50e+00                   -4.80e+00  \n       vehicleLotus Europa        vehicleMazda RX4 Wag  \n                  1.52e+01                    5.80e+00  \n           vehicleMerc 230             vehicleMerc 280  \n                  7.60e+00                    4.00e+00  \n          vehicleMerc 280C           vehicleMerc 450SL  \n                  2.60e+00                    2.10e+00  \n        vehicleMerc 450SLC        vehiclePorsche 914-2  \n                  9.73e-16                    1.08e+01  \n      vehicleToyota Corona              vehicleValiant  \n                  6.30e+00                    2.90e+00  \n         vehicleVolvo 142E                        disp  \n                  6.20e+00                          NA  \n                        hp                        drat  \n                        NA                          NA  \n                        wt                        qsec  \n                        NA                          NA  \n                        vs                          am  \n                        NA                          NA  \n                      gear                        carb  \n                        NA                          NA  \n                    cyl_X4                      cyl_X6  \n                        NA                          NA  \n                    cyl_X8  \n                        NA  \n```\n:::\n:::\n\n\nTo fix this, we need to carefully monitor both our preprocessing steps and our model inputs... or we could just feed our preprocessing steps directly into our model and let that dictate what it should do (much easier).\n\nWe'd like the recipe to seamlessly feed into the model. To do that, we need a workflow with a our recipe and a model.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# set up final recipe\nfinal_recipe <- recipe(mpg~.,data = train_data) |> \n  # set some of the variables to \n  update_role(vehicle,am,vs,gear,qsec,drat, new_role = 'id variable') |> \n  # set cyl as factor\n  step_mutate(cyl = as.factor(cyl)) |>\n  # dummy step cyl\n  step_dummy(cyl, one_hot = TRUE) |> \n  # take log of hp\n  step_log(hp)\n\n# set model engine\nlm_model <- linear_reg() |> set_engine('lm')\n\n# create workflow object\nexample_workflow <- workflow() |> \n  # add recipe\n  add_recipe(final_recipe) |> \n  # add model\n  add_model(lm_model)\n\n# prepare the recipe and estimate the model in a single call\nexample_workflow_fit <- fit(example_workflow, data = train_data)\n\n# print model\nexample_workflow_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n== Workflow [trained] ==========================================================\nPreprocessor: Recipe\nModel: linear_reg()\n\n-- Preprocessor ----------------------------------------------------------------\n3 Recipe Steps\n\n* step_mutate()\n* step_dummy()\n* step_log()\n\n-- Model -----------------------------------------------------------------------\n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n(Intercept)         disp           hp           wt         carb       cyl_X4  \n   65.76710      0.01709     -8.35797     -3.86068      0.80086      3.27176  \n     cyl_X6       cyl_X8  \n    0.41508           NA  \n```\n:::\n:::\n\n\nOkay, our model is trained. Let's see how it did on the test data. Not bad. Maybe a little heteroskedasticity, but a good first attempt.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# collect predictions on test data\ndata_test_preds <- predict(example_workflow_fit, new_data = test_data) |> \n  bind_cols(test_data) |> \n  select(vehicle,mpg,cyl,hp,.pred)\n\n# print predictions\ndata_test_preds |> knitr::kable()\n```\n\n::: {.cell-output-display}\n|vehicle          |  mpg| cyl|  hp|    .pred|\n|:----------------|----:|---:|---:|--------:|\n|Mazda RX4        | 21.0|   6| 110| 22.71879|\n|Merc 240D        | 24.4|   4|  62| 26.33785|\n|Merc 450SE       | 16.4|   8| 180| 13.76795|\n|Honda Civic      | 30.4|   4|  52| 32.67503|\n|Toyota Corolla   | 33.9|   4|  65| 29.08117|\n|Pontiac Firebird | 19.2|   8| 175| 16.19396|\n|Ford Pantera L   | 15.8|   8| 264| 16.12766|\n|Ferrari Dino     | 19.7|   6| 175| 19.60437|\n|Maserati Bora    | 15.0|   8| 335| 14.94153|\n:::\n\n```{.r .cell-code}\n# plot check\nggplot(data_test_preds, aes(x=mpg, y = .pred)) +\ngeom_abline() +\ngeom_point() + \nlabs(title='Test Data, Estimated MPG', x='Original',y='Estimated') +\ntheme_bw()\n```\n\n::: {.cell-output-display}\n![](tidymodels_recipes_files/figure-html/unnamed-chunk-56-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# rsq check\nrsq(data_test_preds,mpg,.pred)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rsq     standard       0.869\n```\n:::\n:::\n\n\nTo learn more about workflows, visit <https://workflows.tidymodels.org/> and check out their [getting started](https://workflows.tidymodels.org/articles/extras/getting-started.html#the-test-set) information.\n\nOnce you get the hang of recipes and building simple workflows, it's a small step into resampling, parameter optimization, and improved model evaluation - all within the tidymodels framework.\n",
    "supporting": [
      "tidymodels_recipes_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}